{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "deep_generative_model_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [
          0
        ],
        "id": "wEJXnNF8OfAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "from itertools import repeat, cycle\n",
        "from functools import reduce\n",
        "from operator import __or__\n",
        "\n",
        "#cuda = torch.cuda.is_available()\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "#from urllib import request\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY-jH9ICOjqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_standard_gaussian(x):\n",
        "    \"\"\"\n",
        "    Evaluates the log pdf of a standard normal distribution at x.\n",
        "\n",
        "    :param x: point to evaluate\n",
        "    :return: log N(x|0,I)\n",
        "    \"\"\"\n",
        "    return torch.sum(-0.5 * math.log(2 * math.pi) - x ** 2 / 2, dim=-1)\n",
        "\n",
        "\n",
        "def log_gaussian(x, mu, log_var):\n",
        "    \"\"\"\n",
        "    Returns the log pdf of a normal distribution parametrised\n",
        "    by mu and log_var evaluated at x.\n",
        "\n",
        "    :param x: point to evaluate\n",
        "    :param mu: mean of distribution\n",
        "    :param log_var: log variance of distribution\n",
        "    :return: log N(x|µ,σ)\n",
        "    \"\"\"\n",
        "    log_pdf = - 0.5 * math.log(2 * math.pi) - log_var / 2 - (x - mu)**2 / (2 * torch.exp(log_var))\n",
        "    return torch.sum(log_pdf, dim=-1)\n",
        "\n",
        "\n",
        "def log_standard_categorical(p):\n",
        "    \"\"\"\n",
        "    Calculates the cross entropy between a (one-hot) categorical vector\n",
        "    and a standard (uniform) categorical distribution.\n",
        "\n",
        "    :param p: one-hot categorical distribution\n",
        "    :return: H(p, u)\n",
        "    \"\"\"\n",
        "    # Uniform prior over y\n",
        "    prior = F.softmax(torch.ones_like(p), dim=1)\n",
        "    prior.requires_grad = False\n",
        "\n",
        "    cross_entropy = -torch.sum(p * torch.log(prior + 1e-8), dim=1)\n",
        "\n",
        "    return cross_entropy\n",
        "\n",
        "def enumerate_discrete(x, y_dim):\n",
        "    \"\"\"\n",
        "    Generates a `torch.Tensor` of size batch_size x n_labels of\n",
        "    the given label.\n",
        "    :param x: tensor with batch size to mimic\n",
        "    :param y_dim: number of total labels\n",
        "    :return variable\n",
        "    \"\"\"\n",
        "    def batch(batch_size, label):\n",
        "        labels = (torch.ones(batch_size, 1) * label).type(torch.LongTensor)\n",
        "        y = torch.zeros((batch_size, y_dim))\n",
        "        y.scatter_(1, labels, 1)\n",
        "        return y.type(torch.LongTensor)\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "    generated = torch.cat([batch(batch_size, i) for i in range(y_dim)])\n",
        "\n",
        "    return Variable(generated.float())\n",
        "\n",
        "\n",
        "def onehot(k):\n",
        "    \"\"\"\n",
        "    Converts a number to its one-hot or 1-of-k representation\n",
        "    vector.\n",
        "    :param k: (int) length of vector\n",
        "    :return: onehot function\n",
        "    \"\"\"\n",
        "    def encode(label):\n",
        "        y = torch.zeros(k)\n",
        "        if label < k:\n",
        "            y[label] = 1\n",
        "        return y\n",
        "    return encode\n",
        "\n",
        "\n",
        "def log_sum_exp(tensor, dim=-1, sum_op=torch.sum):\n",
        "    \"\"\"\n",
        "    Uses the LogSumExp (LSE) as an approximation for the sum in a log-domain.\n",
        "    :param tensor: Tensor to compute LSE over\n",
        "    :param dim: dimension to perform operation over\n",
        "    :param sum_op: reductive operation to be applied, e.g. torch.sum or torch.mean\n",
        "    :return: LSE\n",
        "    \"\"\"\n",
        "    max, _ = torch.max(tensor, dim=dim, keepdim=True)\n",
        "    return torch.log(sum_op(torch.exp(tensor - max), dim=dim, keepdim=True) + 1e-8) + max\n",
        "\n",
        "def binary_cross_entropy(r, x):\n",
        "    return -torch.sum(x * torch.log(r + 1e-8) + (1 - x) * torch.log(1 - r + 1e-8), dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxSLep22i8Ex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Stochastic(nn.Module):\n",
        "    \"\"\"\n",
        "    Base stochastic layer that uses the\n",
        "    reparametrization trick [Kingma 2013]\n",
        "    to draw a sample from a distribution\n",
        "    parametrised by mu and log_var.\n",
        "    \"\"\"\n",
        "    def reparametrize(self, mu, log_var):\n",
        "        epsilon = Variable(torch.randn(mu.size()), requires_grad=False)\n",
        "\n",
        "        if mu.is_cuda:\n",
        "            epsilon = epsilon.cuda()\n",
        "        \n",
        "        # log_std = 0.5 * log_var\n",
        "        # std = exp(log_std)\n",
        "        std = log_var.mul(0.5).exp_()\n",
        "\n",
        "        # z = std * epsilon + mu\n",
        "        z = mu.addcmul(std, epsilon)\n",
        "        return z\n",
        "\n",
        "class GaussianSample(Stochastic):\n",
        "    \"\"\"\n",
        "    Layer that represents a sample from a\n",
        "    Gaussian distribution.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(GaussianSample, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        self.mu = nn.Linear(in_features, out_features)\n",
        "        self.log_var = nn.Linear(in_features, out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu = self.mu(x)\n",
        "        log_var = F.softplus(self.log_var(x))\n",
        "\n",
        "        return self.reparametrize(mu, log_var), mu, log_var\n",
        "\n",
        "class GaussianMerge(GaussianSample):\n",
        "    \"\"\"\n",
        "    Precision weighted merging of two Gaussian\n",
        "    distributions.\n",
        "    Merges information from z into the given\n",
        "    mean and log variance and produces\n",
        "    a sample from this new distribution.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(GaussianMerge, self).__init__(in_features, out_features)\n",
        "\n",
        "    def forward(self, z, mu1, log_var1):\n",
        "        # Calculate precision of each distribution\n",
        "        # (inverse variance)\n",
        "        mu2 = self.mu(z)\n",
        "        log_var2 = F.softplus(self.log_var(z))\n",
        "        precision1, precision2 = (1/torch.exp(log_var1), 1/torch.exp(log_var2))\n",
        "\n",
        "        # Merge distributions into a single new\n",
        "        # distribution\n",
        "        mu = ((mu1 * precision1) + (mu2 * precision2)) / (precision1 + precision2)\n",
        "\n",
        "        var = 1 / (precision1 + precision2)\n",
        "        log_var = torch.log(var + 1e-8)\n",
        "\n",
        "        return self.reparametrize(mu, log_var), mu, log_var\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, dims, sample_layer=GaussianSample):\n",
        "        \"\"\"\n",
        "        Inference network\n",
        "\n",
        "        Attempts to infer the probability distribution\n",
        "        p(z|x) from the data by fitting a variational\n",
        "        distribution q_φ(z|x). Returns the two parameters\n",
        "        of the distribution (µ, log σ²).\n",
        "\n",
        "        :param dims: dimensions of the networks\n",
        "           given by the number of neurons on the form\n",
        "           [input_dim, [hidden_dims], latent_dim].\n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        [x_dim, h_dim, z_dim] = dims\n",
        "        neurons = [x_dim, *h_dim]\n",
        "        linear_layers = [nn.Linear(neurons[i-1], neurons[i]) for i in range(1, len(neurons))]\n",
        "\n",
        "        self.hidden = nn.ModuleList(linear_layers)\n",
        "        self.sample = sample_layer(h_dim[-1], z_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.hidden:\n",
        "            x = F.relu(layer(x))\n",
        "        return self.sample(x)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        \"\"\"\n",
        "        Generative network\n",
        "\n",
        "        Generates samples from the original distribution\n",
        "        p(x) by transforming a latent representation, e.g.\n",
        "        by finding p_θ(x|z).\n",
        "\n",
        "        :param dims: dimensions of the networks\n",
        "            given by the number of neurons on the form\n",
        "            [latent_dim, [hidden_dims], input_dim].\n",
        "        \"\"\"\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        [z_dim, h_dim, x_dim] = dims\n",
        "\n",
        "        neurons = [z_dim, *h_dim]\n",
        "        linear_layers = [nn.Linear(neurons[i-1], neurons[i]) for i in range(1, len(neurons))]\n",
        "        self.hidden = nn.ModuleList(linear_layers)\n",
        "\n",
        "        self.reconstruction = nn.Linear(h_dim[-1], x_dim)\n",
        "\n",
        "        self.output_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.hidden:\n",
        "            x = F.relu(layer(x))\n",
        "        return self.output_activation(self.reconstruction(x))\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    # need to change this so conforms to other algorithms layer definition\n",
        "    def __init__(self, dims):\n",
        "        \"\"\"\n",
        "        Single hidden layer classifier\n",
        "        with softmax output.\n",
        "        \"\"\"\n",
        "        super(Classifier, self).__init__()\n",
        "        [x_dim, h_dim, y_dim] = dims\n",
        "        self.dense = nn.Linear(x_dim, h_dim)\n",
        "        self.logits = nn.Linear(h_dim, y_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.dense(x))\n",
        "        x = F.softmax(self.logits(x), dim=-1)\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnfEhO7DqUsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# models\n",
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        \"\"\"\n",
        "        Variational Autoencoder [Kingma 2013] model\n",
        "        consisting of an encoder/decoder pair for which\n",
        "        a variational distribution is fitted to the\n",
        "        encoder. Also known as the M1 model in [Kingma 2014].\n",
        "\n",
        "        :param dims: x, z and hidden dimensions of the networks\n",
        "        \"\"\"\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "\n",
        "        [x_dim, z_dim, h_dim] = dims\n",
        "        self.z_dim = z_dim\n",
        "        self.flow = None\n",
        "\n",
        "        self.encoder = Encoder([x_dim, h_dim, z_dim])\n",
        "        self.decoder = Decoder([z_dim, list(reversed(h_dim)), x_dim])\n",
        "        self.kl_divergence = 0\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                init.xavier_normal(m.weight.data)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "\n",
        "    def _kld(self, z, q_param, p_param=None):\n",
        "        \"\"\"\n",
        "        Computes the KL-divergence of\n",
        "        some element z.\n",
        "\n",
        "        KL(q||p) = -∫ q(z) log [ p(z) / q(z) ]\n",
        "                 = -E[log p(z) - log q(z)]\n",
        "\n",
        "        :param z: sample from q-distribuion\n",
        "        :param q_param: (mu, log_var) of the q-distribution\n",
        "        :param p_param: (mu, log_var) of the p-distribution\n",
        "        :return: KL(q||p)\n",
        "        \"\"\"\n",
        "        (mu, log_var) = q_param\n",
        "\n",
        "        if self.flow is not None:\n",
        "            f_z, log_det_z = self.flow(z)\n",
        "            qz = log_gaussian(z, mu, log_var) - sum(log_det_z)\n",
        "            z = f_z\n",
        "        else:\n",
        "            qz = log_gaussian(z, mu, log_var)\n",
        "\n",
        "        if p_param is None:\n",
        "            pz = log_standard_gaussian(z)\n",
        "        else:\n",
        "            (mu, log_var) = p_param\n",
        "            pz = log_gaussian(z, mu, log_var)\n",
        "\n",
        "        kl = qz - pz\n",
        "\n",
        "        return kl\n",
        "\n",
        "    def add_flow(self, flow):\n",
        "        self.flow = flow\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        \"\"\"\n",
        "        Runs a data point through the model in order\n",
        "        to provide its reconstruction and q distribution\n",
        "        parameters.\n",
        "\n",
        "        :param x: input data\n",
        "        :return: reconstructed input\n",
        "        \"\"\"\n",
        "        z, z_mu, z_log_var = self.encoder(x)\n",
        "\n",
        "        self.kl_divergence = self._kld(z, (z_mu, z_log_var))\n",
        "\n",
        "        x_mu = self.decoder(z)\n",
        "\n",
        "        return x_mu\n",
        "\n",
        "    def sample(self, z):\n",
        "        \"\"\"\n",
        "        Given z ~ N(0, I) generates a sample from\n",
        "        the learned distribution based on p_θ(x|z).\n",
        "        :param z: (torch.autograd.Variable) Random normal variable\n",
        "        :return: (torch.autograd.Variable) generated sample\n",
        "        \"\"\"\n",
        "        return self.decoder(z)\n",
        "\n",
        "\n",
        "class DeepGenerativeModel(VariationalAutoencoder):\n",
        "    def __init__(self, dims):\n",
        "        \"\"\"\n",
        "        M2 code replication from the paper\n",
        "        'Semi-Supervised Learning with Deep Generative Models'\n",
        "        (Kingma 2014) in PyTorch.\n",
        "\n",
        "        The \"Generative semi-supervised model\" is a probabilistic\n",
        "        model that incorporates label information in both\n",
        "        inference and generation.\n",
        "\n",
        "        Initialise a new generative model\n",
        "        :param dims: dimensions of x, y, z and hidden layers.\n",
        "        \"\"\"\n",
        "        [x_dim, self.y_dim, z_dim, h_dim] = dims\n",
        "        super(DeepGenerativeModel, self).__init__([x_dim, z_dim, h_dim])\n",
        "\n",
        "        self.encoder = Encoder([x_dim + self.y_dim, h_dim, z_dim])\n",
        "        self.decoder = Decoder([z_dim + self.y_dim, list(reversed(h_dim)), x_dim])\n",
        "        self.classifier = Classifier([x_dim, h_dim[0], self.y_dim])\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                init.xavier_normal(m.weight.data)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # Add label and data and generate latent variable\n",
        "        z, z_mu, z_log_var = self.encoder(torch.cat([x, y], dim=1))\n",
        "\n",
        "        self.kl_divergence = self._kld(z, (z_mu, z_log_var))\n",
        "\n",
        "        # Reconstruct data point from latent data and label\n",
        "        x_mu = self.decoder(torch.cat([z, y], dim=1))\n",
        "\n",
        "        return x_mu\n",
        "\n",
        "    def classify(self, x):\n",
        "        logits = self.classifier(x)\n",
        "        return logits\n",
        "\n",
        "    def sample(self, z, y):\n",
        "        \"\"\"\n",
        "        Samples from the Decoder to generate an x.\n",
        "        :param z: latent normal variable\n",
        "        :param y: label (one-hot encoded)\n",
        "        :return: x\n",
        "        \"\"\"\n",
        "        y = y.float()\n",
        "        x = self.decoder(torch.cat([z, y], dim=1))\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "26PinGUROfAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_mnist(location, batch_size, labels_per_class):\n",
        "\n",
        "    n_labels = 10\n",
        "\n",
        "    flatten_bernoulli = lambda x: transforms.ToTensor()(x).view(-1).bernoulli()\n",
        "\n",
        "    mnist_train = MNIST(location, train=True, download=True,\n",
        "                        transform=flatten_bernoulli, target_transform=onehot(n_labels))\n",
        "    mnist_valid = MNIST(location, train=False, download=True,\n",
        "                        transform=flatten_bernoulli, target_transform=onehot(n_labels))\n",
        "\n",
        "    def get_sampler(labels, n=None):\n",
        "        # Only choose digits in n_labels\n",
        "        (indices,) = np.where(reduce(__or__, [labels == i for i in np.arange(n_labels)]))\n",
        "\n",
        "        # Ensure uniform distribution of labels\n",
        "        np.random.shuffle(indices)\n",
        "        # reorganize the data by labels and horizontally stack them so the data is like 0..0,1..1,2...2..so on\n",
        "        indices = np.hstack([list(filter(lambda idx: labels[idx] == i, indices))[:n] for i in range(n_labels)])\n",
        "        indices = torch.from_numpy(indices) # convert into a tensor\n",
        "        # Samples elements randomly from a given list of indices, without replacement.\n",
        "        sampler = SubsetRandomSampler(indices)\n",
        "        return sampler\n",
        "\n",
        "    # Dataloaders for MNIST\n",
        "    labelled = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, num_workers=2, \n",
        "                                           sampler=get_sampler(mnist_train.train_labels.numpy(), labels_per_class))\n",
        "    unlabelled = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, num_workers=2, \n",
        "                                             sampler=get_sampler(mnist_train.train_labels.numpy()))\n",
        "    validation = torch.utils.data.DataLoader(mnist_valid, batch_size=batch_size, num_workers=2, \n",
        "                                             sampler=get_sampler(mnist_valid.test_labels.numpy()))\n",
        "\n",
        "    return labelled, unlabelled, validation\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A7lNj4cRBKP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImportanceWeightedSampler(object):\n",
        "    \"\"\"\n",
        "    Importance weighted sampler [Burda 2015] to\n",
        "    be used in conjunction with SVI.\n",
        "    \"\"\"\n",
        "    def __init__(self, mc=1, iw=1):\n",
        "        \"\"\"\n",
        "        Initialise a new sampler.\n",
        "        :param mc: number of Monte Carlo samples\n",
        "        :param iw: number of Importance Weighted samples\n",
        "        \"\"\"\n",
        "        self.mc = mc\n",
        "        self.iw = iw\n",
        "\n",
        "    def resample(self, x):\n",
        "        return x.repeat(self.mc * self.iw, 1)\n",
        "\n",
        "    def __call__(self, elbo):\n",
        "        elbo = elbo.view(self.mc, self.iw, -1)\n",
        "        elbo = torch.mean(log_sum_exp(elbo, dim=1, sum_op=torch.mean), dim=0)\n",
        "        return elbo.view(-1)\n",
        "\n",
        "class SVI(nn.Module):\n",
        "    \"\"\"\n",
        "    Stochastic variational inference (SVI).\n",
        "    \"\"\"\n",
        "    base_sampler = ImportanceWeightedSampler(mc=1, iw=1)\n",
        "    def __init__(self, model, likelihood=F.binary_cross_entropy, beta=repeat(1), sampler=base_sampler):\n",
        "        \"\"\"\n",
        "        Initialises a new SVI optimizer for semi-supervised learning.\n",
        "        :param model: semi-supervised model to evaluate\n",
        "        :param likelihood: p(x|y,z) for example BCE or MSE\n",
        "        :param sampler: sampler for x and y, e.g. for Monte Carlo\n",
        "        :param beta: warm-up/scaling of KL-term\n",
        "        \"\"\"\n",
        "        super(SVI, self).__init__()\n",
        "        self.model = model\n",
        "        self.likelihood = likelihood\n",
        "        self.sampler = sampler\n",
        "        self.beta = beta\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        is_labelled = False if y is None else True\n",
        "\n",
        "        # Prepare for sampling\n",
        "        xs, ys = (x, y)\n",
        "\n",
        "        # Enumerate choices of label\n",
        "        if not is_labelled:\n",
        "            ys = enumerate_discrete(xs, self.model.y_dim)\n",
        "            xs = xs.repeat(self.model.y_dim, 1)\n",
        "\n",
        "        # Increase sampling dimension\n",
        "        xs = self.sampler.resample(xs)\n",
        "        ys = self.sampler.resample(ys)\n",
        "\n",
        "        reconstruction = self.model(xs, ys)\n",
        "\n",
        "        # p(x|y,z)\n",
        "        likelihood = -self.likelihood(reconstruction, xs)\n",
        "\n",
        "        # p(y)\n",
        "        prior = -log_standard_categorical(ys)\n",
        "\n",
        "        # Equivalent to -L(x, y)\n",
        "        elbo = likelihood + prior - next(self.beta) * self.model.kl_divergence\n",
        "        L = self.sampler(elbo)\n",
        "\n",
        "        if is_labelled:\n",
        "            return torch.mean(L)\n",
        "\n",
        "        logits = self.model.classify(x)\n",
        "\n",
        "        L = L.view_as(logits.t()).t()\n",
        "\n",
        "        # Calculate entropy H(q(y|x)) and sum over all labels\n",
        "        H = -torch.sum(torch.mul(logits, torch.log(logits + 1e-8)), dim=-1)\n",
        "        L = torch.sum(torch.mul(logits, L), dim=-1)\n",
        "\n",
        "        # Equivalent to -U(x)\n",
        "        U = L + H\n",
        "        return torch.mean(U)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hisvrr8y4JXc",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate M2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEGjRVr1RDek",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "121e1dca-eb70-46b4-9948-06e7cf86274d"
      },
      "source": [
        "y_dim = 10\n",
        "z_dim = 32\n",
        "h_dim = [256, 128]\n",
        "model = DeepGenerativeModel([784, y_dim, z_dim, h_dim])\n",
        "model"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:112: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeepGenerativeModel(\n",
              "  (encoder): Encoder(\n",
              "    (hidden): ModuleList(\n",
              "      (0): Linear(in_features=794, out_features=256, bias=True)\n",
              "      (1): Linear(in_features=256, out_features=128, bias=True)\n",
              "    )\n",
              "    (sample): GaussianSample(\n",
              "      (mu): Linear(in_features=128, out_features=32, bias=True)\n",
              "      (log_var): Linear(in_features=128, out_features=32, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (hidden): ModuleList(\n",
              "      (0): Linear(in_features=42, out_features=128, bias=True)\n",
              "      (1): Linear(in_features=128, out_features=256, bias=True)\n",
              "    )\n",
              "    (reconstruction): Linear(in_features=256, out_features=784, bias=True)\n",
              "    (output_activation): Sigmoid()\n",
              "  )\n",
              "  (classifier): Classifier(\n",
              "    (dense): Linear(in_features=784, out_features=256, bias=True)\n",
              "    (logits): Linear(in_features=256, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgVZAt_kOfA5",
        "colab_type": "code",
        "outputId": "5d6bee6d-277f-4e19-9bc1-43ac61b954f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Only use N labelled examples per class\n",
        "# The rest of the data is unlabelled.\n",
        "sampler = ImportanceWeightedSampler(mc=1, iw=1)\n",
        "elbo = SVI(model, likelihood=binary_cross_entropy, sampler=sampler)\n",
        "\n",
        "# number of labelled instances per class\n",
        "N_labels = [10, 50, 100, 200, 500, 1000, 2500]\n",
        "BATCH_SIZE = 32\n",
        "accuracys = []\n",
        "\n",
        "for N in N_labels:\n",
        "    model = DeepGenerativeModel([784, y_dim, z_dim, h_dim])   \n",
        "\n",
        "    labelled, unlabelled, validation = get_mnist(location=\"./\", batch_size=BATCH_SIZE, labels_per_class=N)\n",
        "    alpha = 0.1 * len(unlabelled) / len(labelled)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.999))\n",
        "    print(\"Label per class = \", N)\n",
        "\n",
        "    for epoch in range(5):\n",
        "        # tell the model to train\n",
        "        model.train()\n",
        "        total_loss, accuracy = (0, 0)\n",
        "\n",
        "        labelled_iter = iter(labelled)\n",
        "      \n",
        "       # making sure we've seen all labelled cases:\n",
        "       # if no labelled case iter until we see one\n",
        "        for (u, _) in unlabelled:\n",
        "            labelled_elem = next(labelled_iter, None)\n",
        "            if labelled_elem is None:\n",
        "                labelled_iter = iter(labelled)\n",
        "                labelled_elem = next(labelled_iter, None)\n",
        "            (x, y) = labelled_elem\n",
        "\n",
        "        # for (x, y), (u, _) in zip(iter(labelled), iter(unlabelled)):\n",
        "        #     # maybe bounded by the shortest, need to maintain a separate iterator for the labelled data\n",
        "        #     # forward + backward + optimize\n",
        "        #     # Wrap in variables\n",
        "            x, y, u = Variable(x), Variable(y), Variable(u)\n",
        "            \n",
        "            L = -elbo(x, y)\n",
        "            U = -elbo(u) # using unlabelled data\n",
        "            # Add auxiliary classification loss q(y|x)\n",
        "            logits = model.classify(x)\n",
        "            \n",
        "            # Regular cross entropy\n",
        "            classication_loss = torch.sum(y * torch.log(logits + 1e-8), dim=1).mean()\n",
        "\n",
        "            J_alpha = L - alpha * classication_loss + U\n",
        "\n",
        "            J_alpha.backward()\n",
        "            # update params\n",
        "            optimizer.step()\n",
        "\n",
        "            # clear params \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            total_loss += torch.Tensor.item(J_alpha.data)\n",
        "            accuracy += torch.mean((torch.max(logits, 1)[1].data == torch.max(y, 1)[1].data).float())\n",
        "        \n",
        "        # end of each epoch \n",
        "        if epoch % 1 == 0:\n",
        "            # tell the model to test\n",
        "            model.eval()\n",
        "            m = len(unlabelled)\n",
        "            print(\"training\")\n",
        "            print(\"Epoch: {}\".format(epoch+1))\n",
        "            print(\"[Train]\\t\\t J_a: {:.2f}, accuracy: {:.2f}\".format(total_loss / m, accuracy / m))\n",
        "\n",
        "            total_loss, accuracy = (0, 0)\n",
        "\n",
        "            for x, y in validation:\n",
        "                x, y = Variable(x), Variable(y)\n",
        "                L = -elbo(x, y)\n",
        "                U = -elbo(x)\n",
        "\n",
        "                logits = model.classify(x)\n",
        "                classication_loss = -torch.sum(y * torch.log(logits + 1e-8), dim=1).mean()\n",
        "                J_alpha = L + alpha * classication_loss + U\n",
        "                total_loss += torch.Tensor.item(J_alpha.data)\n",
        "                accuracy += torch.mean((torch.max(logits, 1)[1].data == torch.max(y, 1)[1].data).float())\n",
        "\n",
        "            m = len(validation)\n",
        "            print(\"validation\")\n",
        "            print(\"[Validation]\\t J_a: {:.2f}, accuracy: {:.2f}\".format(total_loss / m, accuracy / m ))\n",
        "            accuracys.append(accuracy / m)\n",
        "\n",
        "            return accuracys"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:112: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Label per class =  10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training\n",
            "Epoch: 1\n",
            "[Train]\t\t J_a: 1120.83, accuracy: 0.99\n",
            "validation\n",
            "[Validation]\t J_a: 1160.94, accuracy: 0.74\n",
            "training\n",
            "Epoch: 2\n",
            "[Train]\t\t J_a: 1116.73, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1167.25, accuracy: 0.73\n",
            "training\n",
            "Epoch: 3\n",
            "[Train]\t\t J_a: 1116.73, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1172.30, accuracy: 0.74\n",
            "training\n",
            "Epoch: 4\n",
            "[Train]\t\t J_a: 1116.63, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1175.51, accuracy: 0.74\n",
            "training\n",
            "Epoch: 5\n",
            "[Train]\t\t J_a: 1116.71, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1179.76, accuracy: 0.74\n",
            "Label per class =  50\n",
            "training\n",
            "Epoch: 1\n",
            "[Train]\t\t J_a: 1118.02, accuracy: 0.97\n",
            "validation\n",
            "[Validation]\t J_a: 1122.36, accuracy: 0.86\n",
            "training\n",
            "Epoch: 2\n",
            "[Train]\t\t J_a: 1116.40, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1123.05, accuracy: 0.86\n",
            "training\n",
            "Epoch: 3\n",
            "[Train]\t\t J_a: 1116.39, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1123.81, accuracy: 0.86\n",
            "training\n",
            "Epoch: 4\n",
            "[Train]\t\t J_a: 1116.33, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1124.32, accuracy: 0.87\n",
            "training\n",
            "Epoch: 5\n",
            "[Train]\t\t J_a: 1116.36, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1124.72, accuracy: 0.87\n",
            "Label per class =  100\n",
            "training\n",
            "Epoch: 1\n",
            "[Train]\t\t J_a: 1117.29, accuracy: 0.96\n",
            "validation\n",
            "[Validation]\t J_a: 1118.78, accuracy: 0.89\n",
            "training\n",
            "Epoch: 2\n",
            "[Train]\t\t J_a: 1116.20, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1118.99, accuracy: 0.90\n",
            "training\n",
            "Epoch: 3\n",
            "[Train]\t\t J_a: 1116.19, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1119.05, accuracy: 0.90\n",
            "training\n",
            "Epoch: 4\n",
            "[Train]\t\t J_a: 1116.23, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1119.16, accuracy: 0.91\n",
            "training\n",
            "Epoch: 5\n",
            "[Train]\t\t J_a: 1116.15, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1119.62, accuracy: 0.91\n",
            "Label per class =  200\n",
            "training\n",
            "Epoch: 1\n",
            "[Train]\t\t J_a: 1117.03, accuracy: 0.94\n",
            "validation\n",
            "[Validation]\t J_a: 1117.49, accuracy: 0.91\n",
            "training\n",
            "Epoch: 2\n",
            "[Train]\t\t J_a: 1116.34, accuracy: 0.99\n",
            "validation\n",
            "[Validation]\t J_a: 1117.48, accuracy: 0.92\n",
            "training\n",
            "Epoch: 3\n",
            "[Train]\t\t J_a: 1116.30, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1117.49, accuracy: 0.92\n",
            "training\n",
            "Epoch: 4\n",
            "[Train]\t\t J_a: 1116.31, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1117.52, accuracy: 0.93\n",
            "training\n",
            "Epoch: 5\n",
            "[Train]\t\t J_a: 1116.26, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1117.47, accuracy: 0.92\n",
            "Label per class =  500\n",
            "training\n",
            "Epoch: 1\n",
            "[Train]\t\t J_a: 1116.67, accuracy: 0.91\n",
            "validation\n",
            "[Validation]\t J_a: 1116.71, accuracy: 0.92\n",
            "training\n",
            "Epoch: 2\n",
            "[Train]\t\t J_a: 1116.41, accuracy: 0.97\n",
            "validation\n",
            "[Validation]\t J_a: 1116.55, accuracy: 0.93\n",
            "training\n",
            "Epoch: 3\n",
            "[Train]\t\t J_a: 1116.32, accuracy: 0.99\n",
            "validation\n",
            "[Validation]\t J_a: 1116.75, accuracy: 0.94\n",
            "training\n",
            "Epoch: 4\n",
            "[Train]\t\t J_a: 1116.32, accuracy: 0.99\n",
            "validation\n",
            "[Validation]\t J_a: 1116.63, accuracy: 0.94\n",
            "training\n",
            "Epoch: 5\n",
            "[Train]\t\t J_a: 1116.24, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1116.64, accuracy: 0.95\n",
            "Label per class =  1000\n",
            "training\n",
            "Epoch: 1\n",
            "[Train]\t\t J_a: 1116.48, accuracy: 0.90\n",
            "validation\n",
            "[Validation]\t J_a: 1116.59, accuracy: 0.93\n",
            "training\n",
            "Epoch: 2\n",
            "[Train]\t\t J_a: 1116.35, accuracy: 0.96\n",
            "validation\n",
            "[Validation]\t J_a: 1116.69, accuracy: 0.94\n",
            "training\n",
            "Epoch: 3\n",
            "[Train]\t\t J_a: 1116.32, accuracy: 0.98\n",
            "validation\n",
            "[Validation]\t J_a: 1116.50, accuracy: 0.95\n",
            "training\n",
            "Epoch: 4\n",
            "[Train]\t\t J_a: 1116.31, accuracy: 0.99\n",
            "validation\n",
            "[Validation]\t J_a: 1116.54, accuracy: 0.95\n",
            "training\n",
            "Epoch: 5\n",
            "[Train]\t\t J_a: 1116.32, accuracy: 0.99\n",
            "validation\n",
            "[Validation]\t J_a: 1116.58, accuracy: 0.96\n",
            "Label per class =  2500\n",
            "training\n",
            "Epoch: 1\n",
            "[Train]\t\t J_a: 1116.34, accuracy: 0.89\n",
            "validation\n",
            "[Validation]\t J_a: 1116.46, accuracy: 0.93\n",
            "training\n",
            "Epoch: 2\n",
            "[Train]\t\t J_a: 1116.27, accuracy: 0.95\n",
            "validation\n",
            "[Validation]\t J_a: 1116.34, accuracy: 0.95\n",
            "training\n",
            "Epoch: 3\n",
            "[Train]\t\t J_a: 1116.21, accuracy: 0.96\n",
            "validation\n",
            "[Validation]\t J_a: 1116.48, accuracy: 0.96\n",
            "training\n",
            "Epoch: 4\n",
            "[Train]\t\t J_a: 1116.27, accuracy: 0.97\n",
            "validation\n",
            "[Validation]\t J_a: 1116.43, accuracy: 0.96\n",
            "training\n",
            "Epoch: 5\n",
            "[Train]\t\t J_a: 1116.23, accuracy: 0.98\n",
            "validation\n",
            "[Validation]\t J_a: 1116.58, accuracy: 0.97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytw1H_BwkFFG",
        "colab_type": "code",
        "outputId": "c1dca183-bb44-460b-b853-3bcc5922eb15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "test_error"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor(0.7366),\n",
              " tensor(0.7322),\n",
              " tensor(0.7360),\n",
              " tensor(0.7370),\n",
              " tensor(0.7359),\n",
              " tensor(0.8572),\n",
              " tensor(0.8626),\n",
              " tensor(0.8639),\n",
              " tensor(0.8654),\n",
              " tensor(0.8656),\n",
              " tensor(0.8888),\n",
              " tensor(0.8974),\n",
              " tensor(0.8991),\n",
              " tensor(0.9058),\n",
              " tensor(0.9051),\n",
              " tensor(0.9088),\n",
              " tensor(0.9184),\n",
              " tensor(0.9194),\n",
              " tensor(0.9263),\n",
              " tensor(0.9246),\n",
              " tensor(0.9243),\n",
              " tensor(0.9347),\n",
              " tensor(0.9420),\n",
              " tensor(0.9438),\n",
              " tensor(0.9463),\n",
              " tensor(0.9290),\n",
              " tensor(0.9449),\n",
              " tensor(0.9526),\n",
              " tensor(0.9523),\n",
              " tensor(0.9558),\n",
              " tensor(0.9347),\n",
              " tensor(0.9501),\n",
              " tensor(0.9602),\n",
              " tensor(0.9641),\n",
              " tensor(0.9674)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQJvPRGikw-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracys = np.array([float(i.numpy()) for i in test_error])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LgqPvfrlDqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "error = 1-accuracys[np.arange(4,35,5)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDv7sC2ylJWO",
        "colab_type": "code",
        "outputId": "da69a49b-3d06-4330-8ce4-e4c0da6095d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "error"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.26407748, 0.13438499, 0.09494805, 0.07537937, 0.05371404,\n",
              "       0.04422921, 0.03264779])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBzWWkkfHHLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# just copying the results, don't want to run again. should have stored the results\n",
        "error = [0.26407748, 0.13438499, 0.09494805, 0.07537937, 0.05371404,\n",
        "       0.04422921, 0.03264779]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9oWBw1ZlatQ",
        "colab_type": "code",
        "outputId": "4236aee9-ebe5-49dd-beac-cb37809b8c90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "N_labels = [10, 50, 100, 200, 500, 1000, 2500] # sampled from each class\n",
        "N_labels_scaled = [i*10 for i in N_labels] # times 10 for each class for total\n",
        "plt.plot(N_labels_scaled, error)\n",
        "plt.xlabel(\"labelled samples\")\n",
        "plt.ylabel(\"testing error\")"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'testing error')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39zvdHT-pJLx",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate M1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEOPwzWm4d81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_mnist_VAE(location, batch_size, labels_per_class):\n",
        "\n",
        "    n_labels = 10\n",
        "\n",
        "    flatten_bernoulli = lambda x: transforms.ToTensor()(x).view(-1).bernoulli()\n",
        "\n",
        "    mnist_train = MNIST(location, train=True, download=True,\n",
        "                        transform=flatten_bernoulli, target_transform=onehot(n_labels))\n",
        "    mnist_valid = MNIST(location, train=False, download=True,\n",
        "                        transform=flatten_bernoulli, target_transform=onehot(n_labels))\n",
        "\n",
        "    def get_indices(labels, n=None):\n",
        "        # Only choose digits in n_labels\n",
        "        (indices,) = np.where(reduce(__or__, [labels == i for i in np.arange(n_labels)]))\n",
        "\n",
        "        # Ensure uniform distribution of labels\n",
        "        np.random.shuffle(indices)\n",
        "        # reorganize the data by labels and horizontally stack them so the data is like 0..0,1..1,2...2..so on\n",
        "        indices = np.hstack([list(filter(lambda idx: labels[idx] == i, indices))[:n] for i in range(n_labels)])\n",
        "        indices = torch.from_numpy(indices) # convert into a tensor\n",
        "        return indices\n",
        "    \n",
        "    # Datasets for MNIST, convert those into numpy arrayes for the SVM \n",
        "    indices =  get_indices(mnist_train.train_labels.numpy(), labels_per_class)\n",
        "    x_train = mnist_train.data.numpy()\n",
        "    y_train = mnist_train.targets.numpy()\n",
        "    labelled_train_x = x_train[indices]\n",
        "    #labelled_x = [i.flatten() for i in labelled_x]\n",
        "    labelled_train_y = y_train[indices]\n",
        "    # l,w,h = x_train.shape\n",
        "    # mask = np.zeros(l,dtype=bool) \n",
        "    # mask[indices] = True\n",
        "    # unlabelled_train_x = x_train[~mask]\n",
        "    # #unlabelled_x = [i.flatten() for i in unlabelled_x]\n",
        "    # unlabelled_train_y = y_train[~mask]\n",
        "    train = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, num_workers=2) \n",
        "    x_valid = mnist_valid.data.numpy()\n",
        "    y_valid = mnist_valid.targets.numpy()\n",
        "\n",
        "    return train, labelled_train_x, labelled_train_y, x_valid, y_valid\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTRD1jyNlkff",
        "colab_type": "code",
        "outputId": "dd5aaea4-748b-409c-bd82-ec9302c9ef2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train M1 model Variational encoder\n",
        "\n",
        "# number of labelled instances per class, there are 10000 instances in total in train so N label is smaller\n",
        "N_labels = [10, 50, 100, 200, 300, 400, 500]\n",
        "M1_accuracys = []\n",
        "\n",
        "for n_label in N_labels: \n",
        "    print(\"labels per class: \", n_label)\n",
        "    model = VariationalAutoencoder([784, 32, [256, 128]]) # generative\n",
        "    clf = LogisticRegression(multi_class='multinomial') # discrimative\n",
        "    train, labelled_train_x, labelled_train_y, x_valid, y_valid = get_mnist_VAE(location=\"./\", batch_size=32, labels_per_class=n_label)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.999))\n",
        "\n",
        "    for epoch in range(5):\n",
        "        # tell the model to train\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        # VAE training doesn't require y and we go through all training examples\n",
        "\n",
        "        for (x, y) in iter(train): \n",
        "            \n",
        "            # Generative Training \n",
        "            x = Variable(x)\n",
        "\n",
        "            reconstruction = model(x)\n",
        "            \n",
        "            likelihood = -binary_cross_entropy(reconstruction, x)\n",
        "            elbo = likelihood - model.kl_divergence\n",
        "            \n",
        "            L = -torch.mean(elbo)\n",
        "\n",
        "            L.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            total_loss += torch.Tensor.item(L.data)        \n",
        "        m = len(train)\n",
        "        # at the end of the epoch\n",
        "        print(f\"Epoch: {epoch}\\tL: {total_loss/m:.2f}\")   \n",
        "\n",
        "    # Discriminative Training (train once with all labelled data and z generated by fully trained generator in this epoch)\n",
        "    n, l, w =  labelled_train_x.shape\n",
        "    labelled_train_x.shape = (n, l*w)\n",
        "    labelled_train_x_tensor = torch.from_numpy(labelled_train_x)\n",
        "    # use the trained model to obtain the latent representation of x\n",
        "    x_latent = model(Variable(labelled_train_x_tensor.float())) \n",
        "    class_labels = labelled_train_y\n",
        "    clf.fit(x_latent.detach().numpy(), class_labels)\n",
        "    \n",
        "    ##### validate when all epoches are done\n",
        "    # reshape the labelled x \n",
        "    n, l, w = x_valid.shape\n",
        "    x_valid.shape = (n, l*w)\n",
        "    x_valid_tensor = torch.from_numpy(x_valid)\n",
        "\n",
        "    # use the trained model to obtain the latent representation of x\n",
        "    x_latent = model(Variable(x_valid_tensor.float()))\n",
        "\n",
        "    # use trained clf to predict y \n",
        "    y_pred = clf.predict(x_latent.detach().numpy())\n",
        "\n",
        "    # calculate accuracy\n",
        "    accuracy = sum(y_pred == y_valid)/len(y_valid)\n",
        "    print(\"accuray: \", accuracy)\n",
        "    M1_accuracys.append(accuracy)"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "labels per class:  10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tL: 181.63\n",
            "Epoch: 1\tL: 151.31\n",
            "Epoch: 2\tL: 144.80\n",
            "Epoch: 3\tL: 141.12\n",
            "Epoch: 4\tL: 138.86\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuray:  0.8033\n",
            "labels per class:  50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tL: 181.70\n",
            "Epoch: 1\tL: 151.02\n",
            "Epoch: 2\tL: 144.45\n",
            "Epoch: 3\tL: 141.10\n",
            "Epoch: 4\tL: 138.95\n",
            "accuray:  0.8908\n",
            "labels per class:  100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tL: 181.79\n",
            "Epoch: 1\tL: 151.09\n",
            "Epoch: 2\tL: 144.64\n",
            "Epoch: 3\tL: 141.13\n",
            "Epoch: 4\tL: 138.71\n",
            "accuray:  0.902\n",
            "labels per class:  200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tL: 182.86\n",
            "Epoch: 1\tL: 152.14\n",
            "Epoch: 2\tL: 145.64\n",
            "Epoch: 3\tL: 141.98\n",
            "Epoch: 4\tL: 139.48\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuray:  0.905\n",
            "labels per class:  300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tL: 181.46\n",
            "Epoch: 1\tL: 151.45\n",
            "Epoch: 2\tL: 144.97\n",
            "Epoch: 3\tL: 141.24\n",
            "Epoch: 4\tL: 138.86\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuray:  0.9099\n",
            "labels per class:  400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tL: 181.40\n",
            "Epoch: 1\tL: 151.37\n",
            "Epoch: 2\tL: 144.70\n",
            "Epoch: 3\tL: 141.18\n",
            "Epoch: 4\tL: 138.79\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuray:  0.9093\n",
            "labels per class:  500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tL: 183.15\n",
            "Epoch: 1\tL: 152.16\n",
            "Epoch: 2\tL: 145.14\n",
            "Epoch: 3\tL: 141.20\n",
            "Epoch: 4\tL: 138.69\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuray:  0.9005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwcqJXydSBm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "M1_accuracys = n[0.8033, 0.8908, 0.902, 0.905 ,0.9099,0.9093,0.9005]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNbeWTTs1dOw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "b026c443-6249-446f-fd45-391b0dd47c6c"
      },
      "source": [
        "N_labels = [10, 50, 100, 200, 300, 400, 500]\n",
        "N_labels_scaled = [i*10 for i in N_labels] # 10 for each class so total number of labelled samples should times 10\n",
        "M1_err = [1-i for i in M1_accuracys]\n",
        "plt.plot(N_labels_scaled, M1_err)\n",
        "plt.xlabel(\"labelled samples\")\n",
        "plt.ylabel(\"testing error\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'testing error')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEHCAYAAAC0pdErAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/w9pEJiHCYC1uQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p1gkHbHEU2K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8205880f-3e1e-474d-f266-0a06b768cd75"
      },
      "source": [
        "# run the same for M2\n",
        "# Only use N labelled examples per class\n",
        "# The rest of the data is unlabelled.\n",
        "y_dim = 10\n",
        "z_dim = 32\n",
        "h_dim = [256, 128]\n",
        "\n",
        "# # 28* 28 input images so the input is 784, \n",
        "# # in the decoder, in_feature = 32+10(y_dim) = 42\n",
        "sampler = ImportanceWeightedSampler(mc=1, iw=1)\n",
        "model = DeepGenerativeModel([784, y_dim, z_dim, h_dim])\n",
        "elbo = SVI(model, likelihood=binary_cross_entropy, sampler=sampler)\n",
        "\n",
        "# number of labelled instances per class\n",
        "N_labels = [10, 50, 100, 200, 300, 400, 500]\n",
        "BATCH_SIZE = 32\n",
        "M2_accuracys = []\n",
        "\n",
        "for N in N_labels:\n",
        "    model = DeepGenerativeModel([784, y_dim, z_dim, h_dim])   \n",
        "    labelled, unlabelled, validation = get_mnist(location=\"./\", batch_size=BATCH_SIZE, labels_per_class=N)\n",
        "    alpha = 0.1 * len(unlabelled) / len(labelled)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.999))\n",
        "    print(\"Label per class = \", N)\n",
        "\n",
        "    for epoch in range(5):\n",
        "        # tell the model to train\n",
        "        model.train()\n",
        "        total_loss, accuracy = (0, 0)\n",
        "\n",
        "        labelled_iter = iter(labelled)\n",
        "      \n",
        "       # making sure we've seen all labelled cases:\n",
        "       # if no labelled case iter until we see one\n",
        "        for (u, _) in unlabelled:\n",
        "            labelled_elem = next(labelled_iter, None)\n",
        "            if labelled_elem is None:\n",
        "                labelled_iter = iter(labelled)\n",
        "                labelled_elem = next(labelled_iter, None)\n",
        "            (x, y) = labelled_elem\n",
        "\n",
        "        # for (x, y), (u, _) in zip(iter(labelled), iter(unlabelled)):\n",
        "            \n",
        "        #     # maybe bounded by the shortest, need to maintain a separate iterator for the labelled data\n",
        "\n",
        "\n",
        "        #     # forward + backward + optimize\n",
        "        #     # Wrap in variables\n",
        "            x, y, u = Variable(x), Variable(y), Variable(u)\n",
        "            \n",
        "            L = -elbo(x, y)\n",
        "            U = -elbo(u) # using unlabelled data\n",
        "            # Add auxiliary classification loss q(y|x)\n",
        "            logits = model.classify(x)\n",
        "            \n",
        "            # Regular cross entropy\n",
        "            classication_loss = torch.sum(y * torch.log(logits + 1e-8), dim=1).mean()\n",
        "\n",
        "            J_alpha = L - alpha * classication_loss + U\n",
        "\n",
        "            J_alpha.backward()\n",
        "            # update params\n",
        "            optimizer.step()\n",
        "\n",
        "            # clear params \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            total_loss += torch.Tensor.item(J_alpha.data)\n",
        "            accuracy += torch.mean((torch.max(logits, 1)[1].data == torch.max(y, 1)[1].data).float())\n",
        "        \n",
        "        # end of each epoch \n",
        "        if epoch % 1 == 0:\n",
        "            # tell the model to test\n",
        "            model.eval()\n",
        "            m = len(unlabelled)\n",
        "            print(\"training\")\n",
        "            print(\"Epoch: {}\".format(epoch+1))\n",
        "            print(\"[Train]\\t\\t J_a: {:.2f}, accuracy: {:.2f}\".format(total_loss / m, accuracy / m))\n",
        "\n",
        "            total_loss, accuracy = (0, 0)\n",
        "\n",
        "            for x, y in validation:\n",
        "                x, y = Variable(x), Variable(y)\n",
        "                L = -elbo(x, y)\n",
        "                U = -elbo(x)\n",
        "\n",
        "                logits = model.classify(x)\n",
        "                classication_loss = -torch.sum(y * torch.log(logits + 1e-8), dim=1).mean()\n",
        "                J_alpha = L + alpha * classication_loss + U\n",
        "                total_loss += torch.Tensor.item(J_alpha.data)\n",
        "                accuracy += torch.mean((torch.max(logits, 1)[1].data == torch.max(y, 1)[1].data).float())\n",
        "\n",
        "            m = len(validation)\n",
        "            print(\"validation\")\n",
        "            print(\"[Validation]\\t J_a: {:.2f}, accuracy: {:.2f}\".format(total_loss / m, accuracy / m ))\n",
        "            M2_accuracys.append(accuracy / m)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:112: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Label per class =  10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training\n",
            "Epoch: 1\n",
            "[Train]\t\t J_a: 1115.71, accuracy: 0.99\n",
            "validation\n",
            "[Validation]\t J_a: 1161.82, accuracy: 0.73\n",
            "training\n",
            "Epoch: 2\n",
            "[Train]\t\t J_a: 1111.87, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1168.81, accuracy: 0.73\n",
            "training\n",
            "Epoch: 3\n",
            "[Train]\t\t J_a: 1111.75, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1174.37, accuracy: 0.73\n",
            "training\n",
            "Epoch: 4\n",
            "[Train]\t\t J_a: 1111.73, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1179.39, accuracy: 0.73\n",
            "training\n",
            "Epoch: 5\n",
            "[Train]\t\t J_a: 1111.66, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1184.04, accuracy: 0.73\n",
            "Label per class =  50\n",
            "training\n",
            "Epoch: 1\n",
            "[Train]\t\t J_a: 1113.84, accuracy: 0.97\n",
            "validation\n",
            "[Validation]\t J_a: 1117.94, accuracy: 0.87\n",
            "training\n",
            "Epoch: 2\n",
            "[Train]\t\t J_a: 1111.93, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1118.73, accuracy: 0.87\n",
            "training\n",
            "Epoch: 3\n",
            "[Train]\t\t J_a: 1111.88, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1119.15, accuracy: 0.87\n",
            "training\n",
            "Epoch: 4\n",
            "[Train]\t\t J_a: 1111.86, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1121.51, accuracy: 0.85\n",
            "training\n",
            "Epoch: 5\n",
            "[Train]\t\t J_a: 1111.90, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1120.26, accuracy: 0.87\n",
            "Label per class =  100\n",
            "training\n",
            "Epoch: 1\n",
            "[Train]\t\t J_a: 1113.01, accuracy: 0.96\n",
            "validation\n",
            "[Validation]\t J_a: 1114.48, accuracy: 0.89\n",
            "training\n",
            "Epoch: 2\n",
            "[Train]\t\t J_a: 1111.90, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1114.69, accuracy: 0.89\n",
            "training\n",
            "Epoch: 3\n",
            "[Train]\t\t J_a: 1111.87, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1114.77, accuracy: 0.90\n",
            "training\n",
            "Epoch: 4\n",
            "[Train]\t\t J_a: 1111.86, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1114.89, accuracy: 0.90\n",
            "training\n",
            "Epoch: 5\n",
            "[Train]\t\t J_a: 1111.84, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1115.10, accuracy: 0.90\n",
            "Label per class =  200\n",
            "training\n",
            "Epoch: 1\n",
            "[Train]\t\t J_a: 1112.68, accuracy: 0.94\n",
            "validation\n",
            "[Validation]\t J_a: 1112.76, accuracy: 0.91\n",
            "training\n",
            "Epoch: 2\n",
            "[Train]\t\t J_a: 1112.07, accuracy: 0.99\n",
            "validation\n",
            "[Validation]\t J_a: 1112.97, accuracy: 0.92\n",
            "training\n",
            "Epoch: 3\n",
            "[Train]\t\t J_a: 1111.96, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1112.97, accuracy: 0.93\n",
            "training\n",
            "Epoch: 4\n",
            "[Train]\t\t J_a: 1111.93, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1112.98, accuracy: 0.93\n",
            "training\n",
            "Epoch: 5\n",
            "[Train]\t\t J_a: 1111.92, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1113.14, accuracy: 0.93\n",
            "Label per class =  300\n",
            "training\n",
            "Epoch: 1\n",
            "[Train]\t\t J_a: 1112.51, accuracy: 0.93\n",
            "validation\n",
            "[Validation]\t J_a: 1112.52, accuracy: 0.92\n",
            "training\n",
            "Epoch: 2\n",
            "[Train]\t\t J_a: 1112.04, accuracy: 0.99\n",
            "validation\n",
            "[Validation]\t J_a: 1112.39, accuracy: 0.93\n",
            "training\n",
            "Epoch: 3\n",
            "[Train]\t\t J_a: 1112.01, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1112.68, accuracy: 0.93\n",
            "training\n",
            "Epoch: 4\n",
            "[Train]\t\t J_a: 1111.91, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1112.38, accuracy: 0.93\n",
            "training\n",
            "Epoch: 5\n",
            "[Train]\t\t J_a: 1111.98, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1112.73, accuracy: 0.93\n",
            "Label per class =  400\n",
            "training\n",
            "Epoch: 1\n",
            "[Train]\t\t J_a: 1112.44, accuracy: 0.91\n",
            "validation\n",
            "[Validation]\t J_a: 1112.28, accuracy: 0.92\n",
            "training\n",
            "Epoch: 2\n",
            "[Train]\t\t J_a: 1112.10, accuracy: 0.98\n",
            "validation\n",
            "[Validation]\t J_a: 1112.54, accuracy: 0.93\n",
            "training\n",
            "Epoch: 3\n",
            "[Train]\t\t J_a: 1112.00, accuracy: 0.99\n",
            "validation\n",
            "[Validation]\t J_a: 1112.41, accuracy: 0.94\n",
            "training\n",
            "Epoch: 4\n",
            "[Train]\t\t J_a: 1111.98, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1112.38, accuracy: 0.94\n",
            "training\n",
            "Epoch: 5\n",
            "[Train]\t\t J_a: 1111.94, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1112.31, accuracy: 0.94\n",
            "Label per class =  500\n",
            "training\n",
            "Epoch: 1\n",
            "[Train]\t\t J_a: 1112.30, accuracy: 0.91\n",
            "validation\n",
            "[Validation]\t J_a: 1112.33, accuracy: 0.92\n",
            "training\n",
            "Epoch: 2\n",
            "[Train]\t\t J_a: 1112.00, accuracy: 0.97\n",
            "validation\n",
            "[Validation]\t J_a: 1112.35, accuracy: 0.93\n",
            "training\n",
            "Epoch: 3\n",
            "[Train]\t\t J_a: 1112.02, accuracy: 0.99\n",
            "validation\n",
            "[Validation]\t J_a: 1112.22, accuracy: 0.94\n",
            "training\n",
            "Epoch: 4\n",
            "[Train]\t\t J_a: 1111.97, accuracy: 0.99\n",
            "validation\n",
            "[Validation]\t J_a: 1112.26, accuracy: 0.94\n",
            "training\n",
            "Epoch: 5\n",
            "[Train]\t\t J_a: 1111.89, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1112.34, accuracy: 0.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df8MU0JwPJbj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "outputId": "19f8b100-6fdb-4b98-bc15-7dd2b68a34c2"
      },
      "source": [
        "M2_accuracys"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor(0.7340),\n",
              " tensor(0.7260),\n",
              " tensor(0.7315),\n",
              " tensor(0.7322),\n",
              " tensor(0.7327),\n",
              " tensor(0.8651),\n",
              " tensor(0.8669),\n",
              " tensor(0.8706),\n",
              " tensor(0.8459),\n",
              " tensor(0.8735),\n",
              " tensor(0.8876),\n",
              " tensor(0.8921),\n",
              " tensor(0.8989),\n",
              " tensor(0.8988),\n",
              " tensor(0.9028),\n",
              " tensor(0.9094),\n",
              " tensor(0.9177),\n",
              " tensor(0.9253),\n",
              " tensor(0.9288),\n",
              " tensor(0.9265),\n",
              " tensor(0.9156),\n",
              " tensor(0.9267),\n",
              " tensor(0.9296),\n",
              " tensor(0.9347),\n",
              " tensor(0.9333),\n",
              " tensor(0.9198),\n",
              " tensor(0.9329),\n",
              " tensor(0.9382),\n",
              " tensor(0.9386),\n",
              " tensor(0.9443),\n",
              " tensor(0.9229),\n",
              " tensor(0.9346),\n",
              " tensor(0.9403),\n",
              " tensor(0.9430),\n",
              " tensor(0.9463)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axH33gPMKF-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "M2_acc = np.array([float(i.numpy()) for i in M2_accuracys])\n",
        "M2_acc = M2_acc[np.arange(4,len(M2_acc), 5)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om4TIhjESg1S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c5299bb-7648-4c19-cc84-989368659ed7"
      },
      "source": [
        ""
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.1967, 0.1092, 0.098 , 0.095 , 0.0901, 0.0907, 0.0995])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP0hcu-mHrKJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "fe0dae4d-17e8-45c4-f32f-e3647ad415ce"
      },
      "source": [
        "N_labels = [10, 50, 100, 200, 300, 400, 500]\n",
        "N_labels_scaled = [i*10 for i in N_labels] # 10 for each class so total number of labelled samples should times 10\n",
        "M2_err = 1-M2_acc\n",
        "plt.plot(N_labels_scaled, M2_err, label='M2 error')\n",
        "plt.plot(N_labels_scaled, M1_err, label='M1 error')\n",
        "plt.xlabel(\"labelled samples\")\n",
        "plt.ylabel(\"testing error\")\n",
        "plt.legend()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb0e70f9160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HInEQTZOKCck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlm0yiFvJ_rL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}