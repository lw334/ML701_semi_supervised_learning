{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Mar  1 12:06:06 2020\n",
    "\n",
    "@author: Naveen\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from selfTraining import selfTrainImageModel, selfTrainTextModel, fullModelImage, fullModelText\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(r'C:\\Users\\Naveen\\Google Drive\\Heinz\\Coursework\\Semester 2\\Machine Learning\\Project')\n",
    "#z = gzip.open('./MNIST Data/train-images-idx3-ubyte.gz', 'r')\n",
    "#image_size = 28\n",
    "#num_images = 60000\n",
    "#buf = z.read(image_size * image_size * num_images)\n",
    "#data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "#xTrain = data.reshape(num_images, image_size, image_size)\n",
    "#xTrain = xTrain.reshape(60000, 784)\n",
    "#xTrain = xTrain/255.0\n",
    "#\n",
    "#z = gzip.open('./MNIST Data/train-labels-idx1-ubyte.gz','r')\n",
    "#buf = z.read(num_images)\n",
    "#yTrain = np.frombuffer(buf, dtype = np.uint8).astype(np.int32)\n",
    "#del(data)\n",
    "#del(z)\n",
    "#del(buf)\n",
    "(xTrain, yTrain), (xTest, yTest) = mnist.load_data()\n",
    "xTrain = xTrain.reshape(60000, 784)\n",
    "xTrain = xTrain/255.0\n",
    "xTest = xTest.reshape(10000, 784)\n",
    "xTest = xTest/255.0\n",
    "\n",
    "#xTrain = xTrain[0:5000, :]\n",
    "#yTrain = yTrain[0:5000]\n",
    "print(np.unique(yTrain, return_counts=True))\n",
    "\n",
    "batchSize = 32\n",
    "maxIter = 5\n",
    "pThresh = 0.9\n",
    "labeledSize = 100\n",
    "selectionType = \"Group\"\n",
    "\n",
    "labelSize = [100, 500, 1000, 2000, 5000, 10000, 25000]\n",
    "selfTrainErr = []\n",
    "for size in labelSize:\n",
    "    output = selfTrainImageModel(xTrain, yTrain, batchSize, maxIter, pThresh, size, selectionType)\n",
    "    wrongLabels = output[1]\n",
    "    yHat = output[0].predict_classes(xTest, batch_size = batchSize)\n",
    "    cm = confusion_matrix(yTest, yHat)\n",
    "    selfTrainErr = np.append(selfTrainErr, 1 - sum(np.diag(cm))/sum(sum(cm)))\n",
    "    print(wrongLabels)\n",
    "    \n",
    "model = fullModelImage(xTrain, yTrain, batchSize)\n",
    "yHat = model.predict_classes(xTest, batch_size = batchSize)\n",
    "cm = confusion_matrix(yTest, yHat)\n",
    "fullErr = 1 - sum(np.diag(cm))/sum(sum(cm))\n",
    "print(fullErr)\n",
    "fullErr = np.repeat(fullErr, len(labelSize))\n",
    "xIdx = list(range(len(labelSize)))\n",
    "plt.plot(xIdx, selfTrainErr, label = \"0% wrong labels\")\n",
    "#plt.plot(xIdx, tempErrorTen, label = \"10% wrong labels\")\n",
    "#plt.plot(xIdx, tempErrorTwenty, label = \"20% wrong labels\")\n",
    "#plt.plot(xIdx, tempErrorTwentyFive, label = \"25% wrong labels\")\n",
    "#plt.plot(xIdx, tempErrorThirtyThree, label = \"33% wrong labels\")\n",
    "plt.plot(xIdx, fullErr, label = \"Supervised Learning\")\n",
    "plt.xticks(xIdx, labelSize)\n",
    "plt.xlabel(\"Number of Labeled Samples\")\n",
    "plt.ylabel(\"Test Error\")\n",
    "plt.title('Performance of Self-training on MNIST Data')\n",
    "plt.legend()\n",
    "plt.savefig('Perf-Self-train-MNIST.png')\n",
    "\n",
    "def plotRegMNISTModels(layerSize):\n",
    "    (xTrain, yTrain), (xTest, yTest) = mnist.load_data()\n",
    "    xTrain = xTrain.reshape(60000, 784)\n",
    "    xTrain = xTrain/255.0\n",
    "    xTest = xTest.reshape(10000, 784)\n",
    "    xTest = xTest/255.0\n",
    "    \n",
    "    batchSize = 32\n",
    "    dropout = [0, 0.05, 0.1, 0.2, 0.25, 0.3, 0.4, 0.5]\n",
    "    regLambda = 0\n",
    "    errors = []\n",
    "    for dopoutPct in dropout:\n",
    "        model = fullModelImage(xTrain, yTrain, batchSize, layerSize, dopoutPct, regLambda)\n",
    "        yHat = model.predict_classes(xTest, batch_size = batchSize)\n",
    "        cm = confusion_matrix(yTest, yHat)\n",
    "        fullErr = 1 - sum(np.diag(cm))/sum(sum(cm))\n",
    "        print(fullErr)\n",
    "        fullErr = np.repeat(fullErr, len(labelSize))\n",
    "        errors.append(fullErr)\n",
    "    xIdx = list(range(len(labelSize)))\n",
    "    for i in range(len(errors)):\n",
    "        aStr = 'Dropout Pct - ' + str(int(dropout[i] * 100))\n",
    "        plt.plot(xIdx, errors[i], label = aStr)\n",
    "    plt.xlabel(\"Dropout Percentage\")\n",
    "    plt.ylabel(\"Test Error\")\n",
    "    plt.title('Self-training on MNIST Data - Dropout')\n",
    "    plt.legend()\n",
    "    \n",
    "plotRegMNISTModels(100)\n",
    "\n",
    "def selfTrainMNIST(pThresh, selectionType, labelSize, induceError, errorDenom, layerSize,\n",
    "                   dopoutPct, regLambda):\n",
    "    #1. Label Sizes\n",
    "    #2. Probabilities\n",
    "    #3. Wrong Label Percent\n",
    "    #4. Model hidden layer size\n",
    "    (xTrain, yTrain), (xTest, yTest) = mnist.load_data()\n",
    "    xTrain = xTrain.reshape(60000, 784)\n",
    "    xTrain = xTrain/255.0\n",
    "    xTest = xTest.reshape(10000, 784)\n",
    "    xTest = xTest/255.0\n",
    "    \n",
    "    #xTrain = xTrain[0:5000, :]\n",
    "    #yTrain = yTrain[0:5000]\n",
    "    print(np.unique(yTrain, return_counts=True))\n",
    "    \n",
    "    batchSize = 32\n",
    "    maxIter = 5\n",
    "    \n",
    "    selfTrainErr = []\n",
    "    for size in labelSize:\n",
    "        output = selfTrainImageModel(xTrain, yTrain, batchSize, maxIter, pThresh, size, selectionType, \n",
    "                                     induceError, errorDenom, layerSize, dopoutPct, regLambda)\n",
    "        wrongLabels = output[1]\n",
    "        yHat = output[0].predict_classes(xTest, batch_size = batchSize)\n",
    "        cm = confusion_matrix(yTest, yHat)\n",
    "        selfTrainErr = np.append(selfTrainErr, 1 - sum(np.diag(cm))/sum(sum(cm)))\n",
    "        print('Denom ', errorDenom, 'Label Size ', size, wrongLabels)\n",
    "        \n",
    "    model = fullModelImage(xTrain, yTrain, batchSize, layerSize, dopoutPct, regLambda)\n",
    "    yHat = model.predict_classes(xTest, batch_size = batchSize)\n",
    "    cm = confusion_matrix(yTest, yHat)\n",
    "    fullErr = 1 - sum(np.diag(cm))/sum(sum(cm))\n",
    "    print(fullErr)\n",
    "    fullErr = np.repeat(fullErr, len(labelSize))\n",
    "    return([selfTrainErr, fullErr])\n",
    "    \n",
    "def plotBaseModels(pThresh, selectionType, labelSize, induceError, errorDenom, pltName):\n",
    "    layerSize = 100\n",
    "    base512errors = selfTrainMNIST(pThresh, selectionType, labelSize, induceError, errorDenom, \n",
    "                                   layerSize, 0.25, 0)\n",
    "    layerSize = 100\n",
    "    base100errors = selfTrainMNIST(pThresh, selectionType, labelSize, induceError, errorDenom, \n",
    "                                   layerSize, 0, 0.01)\n",
    "    xIdx = list(range(len(labelSize)))\n",
    "    plt.plot(xIdx, base512errors[0], label = \"Dropout - 25%\")\n",
    "    plt.plot(xIdx, base512errors[1], label = \"Supervised Learning - Dropout - 25%\")\n",
    "    plt.plot(xIdx, base100errors[0], label = \"L2 Reg - lambda 0.01\")\n",
    "    plt.plot(xIdx, base100errors[1], label = \"Supervised Learning - L2 Reg - lambda 0.01\")\n",
    "    plt.xticks(xIdx, labelSize)\n",
    "    plt.xlabel(\"Number of Labeled Samples\")\n",
    "    plt.ylabel(\"Test Error\")\n",
    "    plt.title('Performance of Self-training on MNIST Data')\n",
    "    plt.legend()\n",
    "    plt.savefig(pltName + '_Self_train_MNIST.png')\n",
    "    \n",
    "def plotWrongLabelModels(pThresh, selectionType, labelSize, induceError, errorDenom, pltName):\n",
    "    layerSize = 100\n",
    "    wrongLabelErrors = []\n",
    "    errors = selfTrainMNIST(pThresh, selectionType, labelSize, False, 0, layerSize)\n",
    "    wrongLabelErrors.append([errors[0].tolist()])\n",
    "    if induceError == True:\n",
    "        for denom in errorDenom:\n",
    "            errors = selfTrainMNIST(pThresh, selectionType, labelSize, induceError, denom, layerSize)\n",
    "            wrongLabelErrors.append([errors[0].tolist()])\n",
    "            #wrongLabelErrors = np.append(wrongLabelErrors, errors[0])\n",
    "    print(wrongLabelErrors)\n",
    "    print(errors[1])\n",
    "    xIdx = list(range(len(labelSize)))\n",
    "    plt.plot(xIdx, wrongLabelErrors[0][0], label = \"0% wrong labels\")\n",
    "    if induceError == True:\n",
    "        for i in range(len(errorDenom)):\n",
    "            label = str(int(100/errorDenom[i]))\n",
    "            plt.plot(xIdx, wrongLabelErrors[1 + i][0], label = label + \"% wrong labels\")\n",
    "    plt.plot(xIdx, errors[1], label = \"Supervised Learning\")\n",
    "    plt.xticks(xIdx, labelSize)\n",
    "    plt.xlabel(\"Number of Labeled Samples\")\n",
    "    plt.ylabel(\"Test Error\")\n",
    "    plt.title('Performance of Self-training on MNIST Data')\n",
    "    plt.legend()\n",
    "    plt.savefig(pltName + '_Self_train_MNIST.png')\n",
    "    return([wrongLabelErrors, errors[1]])\n",
    "    \n",
    "pThresh = 0.9\n",
    "selectionType = \"Group\"\n",
    "labelSize = [100, 500, 1000, 2000, 5000, 10000, 25000]\n",
    "induceError = False\n",
    "errorDenom = 10\n",
    "pltName = 'RegvsDropout'\n",
    "plotBaseModels(pThresh, selectionType, labelSize, induceError, errorDenom, pltName)\n",
    "\n",
    "induceError = True\n",
    "labelSize = [100, 500, 1000, 2000]\n",
    "errorDenom = [20, 10, 5, 4, 2]\n",
    "pltName = 'Wrong_Labels'\n",
    "wrongLabelErrors = plotWrongLabelModels(pThresh, selectionType, labelSize, induceError, \n",
    "                                        errorDenom, pltName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "xTrain shape: (25000, 80)\n",
      "xTest shape: (25000, 80)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "fullModelText() missing 2 required positional arguments: 'dropout' and 'regLambda'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0442c7c8b25d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'xTest shape:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxTest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfullModelText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxTest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myTest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test score:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: fullModelText() missing 2 required positional arguments: 'dropout' and 'regLambda'"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "###################### IMDB using LSTM ######################\n",
    "#############################################################\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(xTrain, yTrain), (xTest, yTest) = imdb.load_data(num_words = max_features)\n",
    "print(len(xTrain), 'train sequences')\n",
    "print(len(xTest), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "xTrain = sequence.pad_sequences(xTrain, maxlen=maxlen)\n",
    "xTest = sequence.pad_sequences(xTest, maxlen=maxlen)\n",
    "print('xTrain shape:', xTrain.shape)\n",
    "print('xTest shape:', xTest.shape)\n",
    "\n",
    "#model = fullModelText(xTrain, yTrain, batchSize)\n",
    "#score, acc = model.evaluate(xTest, yTest, batch_size = batch_size)\n",
    "#print('Test score:', score)\n",
    "#print('Test accuracy:', acc)\n",
    "\n",
    "\n",
    "batchSize = 32\n",
    "maxIter = 5\n",
    "pThresh = 0.95\n",
    "labeledSize = 100\n",
    "selectionType = \"Group\"\n",
    "dataType = 'text'\n",
    "maxFeatures = 20000\n",
    "\n",
    "dropout = 0.2\n",
    "regLambda = 0\n",
    "labelSize = [1000, 2000, 5000, 10000, 20000]\n",
    "selfTrainErr = []\n",
    "for size in labelSize:\n",
    "    output = selfTrainTextModel(xTrain, yTrain, batchSize, maxIter, pThresh, size, selectionType,\n",
    "                                maxFeatures, dropout, regLambda)\n",
    "    wrongLabels = output[1]\n",
    "    yHat = output[0].predict_classes(xTest, batch_size = batchSize)\n",
    "    cm = confusion_matrix(yTest, yHat)\n",
    "    selfTrainErr = np.append(selfTrainErr, 1 - sum(np.diag(cm))/sum(sum(cm)))\n",
    "    print(wrongLabels)\n",
    "    \n",
    "model = fullModelText(xTrain, yTrain, batchSize, dropout, regLambda)\n",
    "yHat = model.predict_classes(xTest, batch_size = batchSize)\n",
    "cm = confusion_matrix(yTest, yHat)\n",
    "fullErr = 1 - sum(np.diag(cm))/sum(sum(cm))\n",
    "print(fullErr)\n",
    "fullErr = np.repeat(fullErr, len(labelSize))\n",
    "xIdx = list(range(len(labelSize)))\n",
    "plt.plot(xIdx, selfTrainErr, color = 'red', label = \"Self Training\")\n",
    "plt.plot(xIdx, fullErr, color = \"blue\", label = \"Supervised Learning\")\n",
    "plt.xticks(xIdx, labelSize)\n",
    "plt.xlabel(\"Number of Labeled Samples\")\n",
    "plt.ylabel(\"Test Error\")\n",
    "plt.title('Self-training on IMDB Data')\n",
    "plt.legend()\n",
    "plt.savefig('Self-train-IMDB.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
