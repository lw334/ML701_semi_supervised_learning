{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Deep Generative Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [
          0
        ],
        "id": "wEJXnNF8OfAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "import torch\n",
        "cuda = torch.cuda.is_available()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import sys\n",
        "sys.path.append(\"../../semi-supervised\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY-jH9ICOjqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import init\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "\n",
        "def log_standard_gaussian(x):\n",
        "    \"\"\"\n",
        "    Evaluates the log pdf of a standard normal distribution at x.\n",
        "\n",
        "    :param x: point to evaluate\n",
        "    :return: log N(x|0,I)\n",
        "    \"\"\"\n",
        "    return torch.sum(-0.5 * math.log(2 * math.pi) - x ** 2 / 2, dim=-1)\n",
        "\n",
        "\n",
        "def log_gaussian(x, mu, log_var):\n",
        "    \"\"\"\n",
        "    Returns the log pdf of a normal distribution parametrised\n",
        "    by mu and log_var evaluated at x.\n",
        "\n",
        "    :param x: point to evaluate\n",
        "    :param mu: mean of distribution\n",
        "    :param log_var: log variance of distribution\n",
        "    :return: log N(x|µ,σ)\n",
        "    \"\"\"\n",
        "    log_pdf = - 0.5 * math.log(2 * math.pi) - log_var / 2 - (x - mu)**2 / (2 * torch.exp(log_var))\n",
        "    return torch.sum(log_pdf, dim=-1)\n",
        "\n",
        "\n",
        "def log_standard_categorical(p):\n",
        "    \"\"\"\n",
        "    Calculates the cross entropy between a (one-hot) categorical vector\n",
        "    and a standard (uniform) categorical distribution.\n",
        "\n",
        "    :param p: one-hot categorical distribution\n",
        "    :return: H(p, u)\n",
        "    \"\"\"\n",
        "    # Uniform prior over y\n",
        "    prior = F.softmax(torch.ones_like(p), dim=1)\n",
        "    prior.requires_grad = False\n",
        "\n",
        "    cross_entropy = -torch.sum(p * torch.log(prior + 1e-8), dim=1)\n",
        "\n",
        "    return cross_entropy\n",
        "\n",
        "class Stochastic(nn.Module):\n",
        "    \"\"\"\n",
        "    Base stochastic layer that uses the\n",
        "    reparametrization trick [Kingma 2013]\n",
        "    to draw a sample from a distribution\n",
        "    parametrised by mu and log_var.\n",
        "    \"\"\"\n",
        "    def reparametrize(self, mu, log_var):\n",
        "        epsilon = Variable(torch.randn(mu.size()), requires_grad=False)\n",
        "\n",
        "        if mu.is_cuda:\n",
        "            epsilon = epsilon.cuda()\n",
        "\n",
        "        # log_std = 0.5 * log_var\n",
        "        # std = exp(log_std)\n",
        "        std = log_var.mul(0.5).exp_()\n",
        "\n",
        "        # z = std * epsilon + mu\n",
        "        z = mu.addcmul(std, epsilon)\n",
        "\n",
        "        return z\n",
        "\n",
        "class GaussianSample(Stochastic):\n",
        "    \"\"\"\n",
        "    Layer that represents a sample from a\n",
        "    Gaussian distribution.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(GaussianSample, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        self.mu = nn.Linear(in_features, out_features)\n",
        "        self.log_var = nn.Linear(in_features, out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu = self.mu(x)\n",
        "        log_var = F.softplus(self.log_var(x))\n",
        "\n",
        "        return self.reparametrize(mu, log_var), mu, log_var\n",
        "\n",
        "\n",
        "class GaussianMerge(GaussianSample):\n",
        "    \"\"\"\n",
        "    Precision weighted merging of two Gaussian\n",
        "    distributions.\n",
        "    Merges information from z into the given\n",
        "    mean and log variance and produces\n",
        "    a sample from this new distribution.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(GaussianMerge, self).__init__(in_features, out_features)\n",
        "\n",
        "    def forward(self, z, mu1, log_var1):\n",
        "        # Calculate precision of each distribution\n",
        "        # (inverse variance)\n",
        "        mu2 = self.mu(z)\n",
        "        log_var2 = F.softplus(self.log_var(z))\n",
        "        precision1, precision2 = (1/torch.exp(log_var1), 1/torch.exp(log_var2))\n",
        "\n",
        "        # Merge distributions into a single new\n",
        "        # distribution\n",
        "        mu = ((mu1 * precision1) + (mu2 * precision2)) / (precision1 + precision2)\n",
        "\n",
        "        var = 1 / (precision1 + precision2)\n",
        "        log_var = torch.log(var + 1e-8)\n",
        "\n",
        "        return self.reparametrize(mu, log_var), mu, log_var\n",
        "\n",
        "\n",
        "class GumbelSoftmax(Stochastic):\n",
        "    \"\"\"\n",
        "    Layer that represents a sample from a categorical\n",
        "    distribution. Enables sampling and stochastic\n",
        "    backpropagation using the Gumbel-Softmax trick.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, n_distributions):\n",
        "        super(GumbelSoftmax, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.n_distributions = n_distributions\n",
        "\n",
        "        self.logits = nn.Linear(in_features, n_distributions*out_features)\n",
        "\n",
        "    def forward(self, x, tau=1.0):\n",
        "        logits = self.logits(x).view(-1, self.n_distributions)\n",
        "\n",
        "        # variational distribution over categories\n",
        "        softmax = F.softmax(logits, dim=-1) #q_y\n",
        "        sample = self.reparametrize(logits, tau).view(-1, self.n_distributions, self.out_features)\n",
        "        sample = torch.mean(sample, dim=1)\n",
        "\n",
        "        return sample, softmax\n",
        "\n",
        "    def reparametrize(self, logits, tau=1.0):\n",
        "        epsilon = Variable(torch.rand(logits.size()), requires_grad=False)\n",
        "\n",
        "        if logits.is_cuda:\n",
        "            epsilon = epsilon.cuda()\n",
        "\n",
        "        # Gumbel distributed noise\n",
        "        gumbel = -torch.log(-torch.log(epsilon+1e-8)+1e-8)\n",
        "        # Softmax as a continuous approximation of argmax\n",
        "        y = F.softmax((logits + gumbel)/tau, dim=1)\n",
        "        return y\n",
        "\n",
        "\n",
        "class PlanarNormalizingFlow(nn.Module):\n",
        "    \"\"\"\n",
        "    Planar normalizing flow [Rezende & Mohamed 2015].\n",
        "    Provides a tighter bound on the ELBO by giving more expressive\n",
        "    power to the approximate distribution, such as by introducing\n",
        "    covariance between terms.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features):\n",
        "        super(PlanarNormalizingFlow, self).__init__()\n",
        "        self.u = nn.Parameter(torch.randn(in_features))\n",
        "        self.w = nn.Parameter(torch.randn(in_features))\n",
        "        self.b = nn.Parameter(torch.ones(1))\n",
        "\n",
        "    def forward(self, z):\n",
        "        # Create uhat such that it is parallel to w\n",
        "        uw = torch.dot(self.u, self.w)\n",
        "        muw = -1 + F.softplus(uw)\n",
        "        uhat = self.u + (muw - uw) * torch.transpose(self.w, 0, -1) / torch.sum(self.w ** 2)\n",
        "\n",
        "        # Equation 21 - Transform z\n",
        "        zwb = torch.mv(z, self.w) + self.b\n",
        "\n",
        "        f_z = z + (uhat.view(1, -1) * F.tanh(zwb).view(-1, 1))\n",
        "\n",
        "        # Compute the Jacobian using the fact that\n",
        "        # tanh(x) dx = 1 - tanh(x)**2\n",
        "        psi = (1 - F.tanh(zwb)**2).view(-1, 1) * self.w.view(1, -1)\n",
        "        psi_u = torch.mv(psi, uhat)\n",
        "\n",
        "        # Return the transformed output along\n",
        "        # with log determninant of J\n",
        "        logdet_jacobian = torch.log(torch.abs(1 + psi_u) + 1e-8)\n",
        "\n",
        "        return f_z, logdet_jacobian\n",
        "\n",
        "\n",
        "class NormalizingFlows(nn.Module):\n",
        "    \"\"\"\n",
        "    Presents a sequence of normalizing flows as a torch.nn.Module.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, flow_type=PlanarNormalizingFlow, n_flows=1):\n",
        "        super(NormalizingFlows, self).__init__()\n",
        "        self.flows = nn.ModuleList([flow_type(in_features) for _ in range(n_flows)])\n",
        "\n",
        "    def forward(self, z):\n",
        "        log_det_jacobian = []\n",
        "\n",
        "        for flow in self.flows:\n",
        "            z, j = flow(z)\n",
        "            log_det_jacobian.append(j)\n",
        "\n",
        "        return z, sum(log_det_jacobian)\n",
        "\n",
        "class Perceptron(nn.Module):\n",
        "    def __init__(self, dims, activation_fn=F.relu, output_activation=None):\n",
        "        super(Perceptron, self).__init__()\n",
        "        self.dims = dims\n",
        "        self.activation_fn = activation_fn\n",
        "        self.output_activation = output_activation\n",
        "\n",
        "        self.layers = nn.ModuleList(list(map(lambda d: nn.Linear(*d), list(zip(dims, dims[1:])))))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "            if i == len(self.layers)-1 and self.output_activation is not None:\n",
        "                x = self.output_activation(x)\n",
        "            else:\n",
        "                x = self.activation_fn(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, dims, sample_layer=GaussianSample):\n",
        "        \"\"\"\n",
        "        Inference network\n",
        "\n",
        "        Attempts to infer the probability distribution\n",
        "        p(z|x) from the data by fitting a variational\n",
        "        distribution q_φ(z|x). Returns the two parameters\n",
        "        of the distribution (µ, log σ²).\n",
        "\n",
        "        :param dims: dimensions of the networks\n",
        "           given by the number of neurons on the form\n",
        "           [input_dim, [hidden_dims], latent_dim].\n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        [x_dim, h_dim, z_dim] = dims\n",
        "        neurons = [x_dim, *h_dim]\n",
        "        linear_layers = [nn.Linear(neurons[i-1], neurons[i]) for i in range(1, len(neurons))]\n",
        "\n",
        "        self.hidden = nn.ModuleList(linear_layers)\n",
        "        self.sample = sample_layer(h_dim[-1], z_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.hidden:\n",
        "            x = F.relu(layer(x))\n",
        "        return self.sample(x)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        \"\"\"\n",
        "        Generative network\n",
        "\n",
        "        Generates samples from the original distribution\n",
        "        p(x) by transforming a latent representation, e.g.\n",
        "        by finding p_θ(x|z).\n",
        "\n",
        "        :param dims: dimensions of the networks\n",
        "            given by the number of neurons on the form\n",
        "            [latent_dim, [hidden_dims], input_dim].\n",
        "        \"\"\"\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        [z_dim, h_dim, x_dim] = dims\n",
        "\n",
        "        neurons = [z_dim, *h_dim]\n",
        "        linear_layers = [nn.Linear(neurons[i-1], neurons[i]) for i in range(1, len(neurons))]\n",
        "        self.hidden = nn.ModuleList(linear_layers)\n",
        "\n",
        "        self.reconstruction = nn.Linear(h_dim[-1], x_dim)\n",
        "\n",
        "        self.output_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.hidden:\n",
        "            x = F.relu(layer(x))\n",
        "        return self.output_activation(self.reconstruction(x))\n",
        "\n",
        "\n",
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        \"\"\"\n",
        "        Variational Autoencoder [Kingma 2013] model\n",
        "        consisting of an encoder/decoder pair for which\n",
        "        a variational distribution is fitted to the\n",
        "        encoder. Also known as the M1 model in [Kingma 2014].\n",
        "\n",
        "        :param dims: x, z and hidden dimensions of the networks\n",
        "        \"\"\"\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "\n",
        "        [x_dim, z_dim, h_dim] = dims\n",
        "        self.z_dim = z_dim\n",
        "        self.flow = None\n",
        "\n",
        "        self.encoder = Encoder([x_dim, h_dim, z_dim])\n",
        "        self.decoder = Decoder([z_dim, list(reversed(h_dim)), x_dim])\n",
        "        self.kl_divergence = 0\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                init.xavier_normal(m.weight.data)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "\n",
        "    def _kld(self, z, q_param, p_param=None):\n",
        "        \"\"\"\n",
        "        Computes the KL-divergence of\n",
        "        some element z.\n",
        "\n",
        "        KL(q||p) = -∫ q(z) log [ p(z) / q(z) ]\n",
        "                 = -E[log p(z) - log q(z)]\n",
        "\n",
        "        :param z: sample from q-distribuion\n",
        "        :param q_param: (mu, log_var) of the q-distribution\n",
        "        :param p_param: (mu, log_var) of the p-distribution\n",
        "        :return: KL(q||p)\n",
        "        \"\"\"\n",
        "        (mu, log_var) = q_param\n",
        "\n",
        "        if self.flow is not None:\n",
        "            f_z, log_det_z = self.flow(z)\n",
        "            qz = log_gaussian(z, mu, log_var) - sum(log_det_z)\n",
        "            z = f_z\n",
        "        else:\n",
        "            qz = log_gaussian(z, mu, log_var)\n",
        "\n",
        "        if p_param is None:\n",
        "            pz = log_standard_gaussian(z)\n",
        "        else:\n",
        "            (mu, log_var) = p_param\n",
        "            pz = log_gaussian(z, mu, log_var)\n",
        "\n",
        "        kl = qz - pz\n",
        "\n",
        "        return kl\n",
        "\n",
        "    def add_flow(self, flow):\n",
        "        self.flow = flow\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        \"\"\"\n",
        "        Runs a data point through the model in order\n",
        "        to provide its reconstruction and q distribution\n",
        "        parameters.\n",
        "\n",
        "        :param x: input data\n",
        "        :return: reconstructed input\n",
        "        \"\"\"\n",
        "        z, z_mu, z_log_var = self.encoder(x)\n",
        "\n",
        "        self.kl_divergence = self._kld(z, (z_mu, z_log_var))\n",
        "\n",
        "        x_mu = self.decoder(z)\n",
        "\n",
        "        return x_mu\n",
        "\n",
        "    def sample(self, z):\n",
        "        \"\"\"\n",
        "        Given z ~ N(0, I) generates a sample from\n",
        "        the learned distribution based on p_θ(x|z).\n",
        "        :param z: (torch.autograd.Variable) Random normal variable\n",
        "        :return: (torch.autograd.Variable) generated sample\n",
        "        \"\"\"\n",
        "        return self.decoder(z)\n",
        "\n",
        "\n",
        "class GumbelAutoencoder(nn.Module):\n",
        "    def __init__(self, dims, n_samples=100):\n",
        "        super(GumbelAutoencoder, self).__init__()\n",
        "\n",
        "        [x_dim, z_dim, h_dim] = dims\n",
        "        self.z_dim = z_dim\n",
        "        self.n_samples = n_samples\n",
        "\n",
        "        self.encoder = Perceptron([x_dim, *h_dim])\n",
        "        self.sampler = GumbelSoftmax(h_dim[-1], z_dim, n_samples)\n",
        "        self.decoder = Perceptron([z_dim, *reversed(h_dim), x_dim], output_activation=F.sigmoid)\n",
        "\n",
        "        self.kl_divergence = 0\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                init.xavier_normal(m.weight.data)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "\n",
        "    def _kld(self, qz):\n",
        "        k = Variable(torch.FloatTensor([self.z_dim]), requires_grad=False)\n",
        "        kl = qz * (torch.log(qz + 1e-8) - torch.log(1.0/k))\n",
        "        kl = kl.view(-1, self.n_samples, self.z_dim)\n",
        "        return torch.sum(torch.sum(kl, dim=1), dim=1)\n",
        "\n",
        "    def forward(self, x, y=None, tau=1):\n",
        "        x = self.encoder(x)\n",
        "\n",
        "        sample, qz = self.sampler(x, tau)\n",
        "        self.kl_divergence = self._kld(qz)\n",
        "\n",
        "        x_mu = self.decoder(sample)\n",
        "\n",
        "        return x_mu\n",
        "\n",
        "    def sample(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "\n",
        "class LadderEncoder(nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        \"\"\"\n",
        "        The ladder encoder differs from the standard encoder\n",
        "        by using batch-normalization and LReLU activation.\n",
        "        Additionally, it also returns the transformation x.\n",
        "\n",
        "        :param dims: dimensions [input_dim, [hidden_dims], [latent_dims]].\n",
        "        \"\"\"\n",
        "        super(LadderEncoder, self).__init__()\n",
        "        [x_dim, h_dim, self.z_dim] = dims\n",
        "        self.in_features = x_dim\n",
        "        self.out_features = h_dim\n",
        "\n",
        "        self.linear = nn.Linear(x_dim, h_dim)\n",
        "        self.batchnorm = nn.BatchNorm1d(h_dim)\n",
        "        self.sample = GaussianSample(h_dim, self.z_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = F.leaky_relu(self.batchnorm(x), 0.1)\n",
        "        return x, self.sample(x)\n",
        "\n",
        "\n",
        "class LadderDecoder(nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        \"\"\"\n",
        "        The ladder dencoder differs from the standard encoder\n",
        "        by using batch-normalization and LReLU activation.\n",
        "        Additionally, it also returns the transformation x.\n",
        "\n",
        "        :param dims: dimensions of the networks\n",
        "            given by the number of neurons on the form\n",
        "            [latent_dim, [hidden_dims], input_dim].\n",
        "        \"\"\"\n",
        "        super(LadderDecoder, self).__init__()\n",
        "\n",
        "        [self.z_dim, h_dim, x_dim] = dims\n",
        "\n",
        "        self.linear1 = nn.Linear(x_dim, h_dim)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(h_dim)\n",
        "        self.merge = GaussianMerge(h_dim, self.z_dim)\n",
        "\n",
        "        self.linear2 = nn.Linear(x_dim, h_dim)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(h_dim)\n",
        "        self.sample = GaussianSample(h_dim, self.z_dim)\n",
        "\n",
        "    def forward(self, x, l_mu=None, l_log_var=None):\n",
        "        if l_mu is not None:\n",
        "            # Sample from this encoder layer and merge\n",
        "            z = self.linear1(x)\n",
        "            z = F.leaky_relu(self.batchnorm1(z), 0.1)\n",
        "            q_z, q_mu, q_log_var = self.merge(z, l_mu, l_log_var)\n",
        "\n",
        "        # Sample from the decoder and send forward\n",
        "        z = self.linear2(x)\n",
        "        z = F.leaky_relu(self.batchnorm2(z), 0.1)\n",
        "        z, p_mu, p_log_var = self.sample(z)\n",
        "\n",
        "        if l_mu is None:\n",
        "            return z\n",
        "\n",
        "        return z, (q_z, (q_mu, q_log_var), (p_mu, p_log_var))\n",
        "\n",
        "\n",
        "class LadderVariationalAutoencoder(VariationalAutoencoder):\n",
        "    def __init__(self, dims):\n",
        "        \"\"\"\n",
        "        Ladder Variational Autoencoder as described by\n",
        "        [Sønderby 2016]. Adds several stochastic\n",
        "        layers to improve the log-likelihood estimate.\n",
        "\n",
        "        :param dims: x, z and hidden dimensions of the networks\n",
        "        \"\"\"\n",
        "        [x_dim, z_dim, h_dim] = dims\n",
        "        super(LadderVariationalAutoencoder, self).__init__([x_dim, z_dim[0], h_dim])\n",
        "\n",
        "        neurons = [x_dim, *h_dim]\n",
        "        encoder_layers = [LadderEncoder([neurons[i - 1], neurons[i], z_dim[i - 1]]) for i in range(1, len(neurons))]\n",
        "        decoder_layers = [LadderDecoder([z_dim[i - 1], h_dim[i - 1], z_dim[i]]) for i in range(1, len(h_dim))][::-1]\n",
        "\n",
        "        self.encoder = nn.ModuleList(encoder_layers)\n",
        "        self.decoder = nn.ModuleList(decoder_layers)\n",
        "        self.reconstruction = Decoder([z_dim[0], h_dim, x_dim])\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                init.xavier_normal(m.weight.data)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Gather latent representation\n",
        "        # from encoders along with final z.\n",
        "        latents = []\n",
        "        for encoder in self.encoder:\n",
        "            x, (z, mu, log_var) = encoder(x)\n",
        "            latents.append((mu, log_var))\n",
        "\n",
        "        latents = list(reversed(latents))\n",
        "\n",
        "        self.kl_divergence = 0\n",
        "        for i, decoder in enumerate([-1, *self.decoder]):\n",
        "            # If at top, encoder == decoder,\n",
        "            # use prior for KL.\n",
        "            l_mu, l_log_var = latents[i]\n",
        "            if i == 0:\n",
        "                self.kl_divergence += self._kld(z, (l_mu, l_log_var))\n",
        "\n",
        "            # Perform downword merge of information.\n",
        "            else:\n",
        "                z, kl = decoder(z, l_mu, l_log_var)\n",
        "                self.kl_divergence += self._kld(*kl)\n",
        "\n",
        "        x_mu = self.reconstruction(z)\n",
        "        return x_mu\n",
        "\n",
        "    def sample(self, z):\n",
        "        for decoder in self.decoder:\n",
        "            z = decoder(z)\n",
        "        return self.reconstruction(z)\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        \"\"\"\n",
        "        Single hidden layer classifier\n",
        "        with softmax output.\n",
        "        \"\"\"\n",
        "        super(Classifier, self).__init__()\n",
        "        [x_dim, h_dim, y_dim] = dims\n",
        "        self.dense = nn.Linear(x_dim, h_dim)\n",
        "        self.logits = nn.Linear(h_dim, y_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.dense(x))\n",
        "        x = F.softmax(self.logits(x), dim=-1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DeepGenerativeModel(VariationalAutoencoder):\n",
        "    def __init__(self, dims):\n",
        "        \"\"\"\n",
        "        M2 code replication from the paper\n",
        "        'Semi-Supervised Learning with Deep Generative Models'\n",
        "        (Kingma 2014) in PyTorch.\n",
        "\n",
        "        The \"Generative semi-supervised model\" is a probabilistic\n",
        "        model that incorporates label information in both\n",
        "        inference and generation.\n",
        "\n",
        "        Initialise a new generative model\n",
        "        :param dims: dimensions of x, y, z and hidden layers.\n",
        "        \"\"\"\n",
        "        [x_dim, self.y_dim, z_dim, h_dim] = dims\n",
        "        super(DeepGenerativeModel, self).__init__([x_dim, z_dim, h_dim])\n",
        "\n",
        "        self.encoder = Encoder([x_dim + self.y_dim, h_dim, z_dim])\n",
        "        self.decoder = Decoder([z_dim + self.y_dim, list(reversed(h_dim)), x_dim])\n",
        "        self.classifier = Classifier([x_dim, h_dim[0], self.y_dim])\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                init.xavier_normal(m.weight.data)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # Add label and data and generate latent variable\n",
        "        z, z_mu, z_log_var = self.encoder(torch.cat([x, y], dim=1))\n",
        "\n",
        "        self.kl_divergence = self._kld(z, (z_mu, z_log_var))\n",
        "\n",
        "        # Reconstruct data point from latent data and label\n",
        "        x_mu = self.decoder(torch.cat([z, y], dim=1))\n",
        "\n",
        "        return x_mu\n",
        "\n",
        "    def classify(self, x):\n",
        "        logits = self.classifier(x)\n",
        "        return logits\n",
        "\n",
        "    def sample(self, z, y):\n",
        "        \"\"\"\n",
        "        Samples from the Decoder to generate an x.\n",
        "        :param z: latent normal variable\n",
        "        :param y: label (one-hot encoded)\n",
        "        :return: x\n",
        "        \"\"\"\n",
        "        y = y.float()\n",
        "        x = self.decoder(torch.cat([z, y], dim=1))\n",
        "        return x\n",
        "\n",
        "\n",
        "class StackedDeepGenerativeModel(DeepGenerativeModel):\n",
        "    def __init__(self, dims, features):\n",
        "        \"\"\"\n",
        "        M1+M2 model as described in [Kingma 2014].\n",
        "\n",
        "        Initialise a new stacked generative model\n",
        "        :param dims: dimensions of x, y, z and hidden layers\n",
        "        :param features: a pretrained M1 model of class `VariationalAutoencoder`\n",
        "            trained on the same dataset.\n",
        "        \"\"\"\n",
        "        [x_dim, y_dim, z_dim, h_dim] = dims\n",
        "        super(StackedDeepGenerativeModel, self).__init__([features.z_dim, y_dim, z_dim, h_dim])\n",
        "\n",
        "        # Be sure to reconstruct with the same dimensions\n",
        "        in_features = self.decoder.reconstruction.in_features\n",
        "        self.decoder.reconstruction = nn.Linear(in_features, x_dim)\n",
        "\n",
        "        # Make vae feature model untrainable by freezing parameters\n",
        "        self.features = features\n",
        "        self.features.train(False)\n",
        "\n",
        "        for param in self.features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # Sample a new latent x from the M1 model\n",
        "        x_sample, _, _ = self.features.encoder(x)\n",
        "\n",
        "        # Use the sample as new input to M2\n",
        "        return super(StackedDeepGenerativeModel, self).forward(x_sample, y)\n",
        "\n",
        "    def classify(self, x):\n",
        "        _, x, _ = self.features.encoder(x)\n",
        "        logits = self.classifier(x)\n",
        "        return logits\n",
        "\n",
        "class AuxiliaryDeepGenerativeModel(DeepGenerativeModel):\n",
        "    def __init__(self, dims):\n",
        "        \"\"\"\n",
        "        Auxiliary Deep Generative Models [Maaløe 2016]\n",
        "        code replication. The ADGM introduces an additional\n",
        "        latent variable 'a', which enables the model to fit\n",
        "        more complex variational distributions.\n",
        "\n",
        "        :param dims: dimensions of x, y, z, a and hidden layers.\n",
        "        \"\"\"\n",
        "        [x_dim, y_dim, z_dim, a_dim, h_dim] = dims\n",
        "        super(AuxiliaryDeepGenerativeModel, self).__init__([x_dim, y_dim, z_dim, h_dim])\n",
        "\n",
        "        self.aux_encoder = Encoder([x_dim, h_dim, a_dim])\n",
        "        self.aux_decoder = Encoder([x_dim + z_dim + y_dim, list(reversed(h_dim)), a_dim])\n",
        "\n",
        "        self.classifier = Classifier([x_dim + a_dim, h_dim[0], y_dim])\n",
        "\n",
        "        self.encoder = Encoder([a_dim + y_dim + x_dim, h_dim, z_dim])\n",
        "        self.decoder = Decoder([y_dim + z_dim, list(reversed(h_dim)), x_dim])\n",
        "\n",
        "    def classify(self, x):\n",
        "        # Auxiliary inference q(a|x)\n",
        "        a, a_mu, a_log_var = self.aux_encoder(x)\n",
        "\n",
        "        # Classification q(y|a,x)\n",
        "        logits = self.classifier(torch.cat([x, a], dim=1))\n",
        "        return logits\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \"\"\"\n",
        "        Forward through the model\n",
        "        :param x: features\n",
        "        :param y: labels\n",
        "        :return: reconstruction\n",
        "        \"\"\"\n",
        "        # Auxiliary inference q(a|x)\n",
        "        q_a, q_a_mu, q_a_log_var = self.aux_encoder(x)\n",
        "\n",
        "        # Latent inference q(z|a,y,x)\n",
        "        z, z_mu, z_log_var = self.encoder(torch.cat([x, y, q_a], dim=1))\n",
        "\n",
        "        # Generative p(x|z,y)\n",
        "        x_mu = self.decoder(torch.cat([z, y], dim=1))\n",
        "\n",
        "        # Generative p(a|z,y,x)\n",
        "        p_a, p_a_mu, p_a_log_var = self.aux_decoder(torch.cat([x, y, z], dim=1))\n",
        "\n",
        "        a_kl = self._kld(q_a, (q_a_mu, q_a_log_var), (p_a_mu, p_a_log_var))\n",
        "        z_kl = self._kld(z, (z_mu, z_log_var))\n",
        "\n",
        "        self.kl_divergence = a_kl + z_kl\n",
        "\n",
        "        return x_mu\n",
        "\n",
        "\n",
        "class LadderDeepGenerativeModel(DeepGenerativeModel):\n",
        "    def __init__(self, dims):\n",
        "        \"\"\"\n",
        "        Ladder version of the Deep Generative Model.\n",
        "        Uses a hierarchical representation that is\n",
        "        trained end-to-end to give very nice disentangled\n",
        "        representations.\n",
        "\n",
        "        :param dims: dimensions of x, y, z layers and h layers\n",
        "            note that len(z) == len(h).\n",
        "        \"\"\"\n",
        "        [x_dim, y_dim, z_dim, h_dim] = dims\n",
        "        super(LadderDeepGenerativeModel, self).__init__([x_dim, y_dim, z_dim[0], h_dim])\n",
        "\n",
        "        neurons = [x_dim, *h_dim]\n",
        "        encoder_layers = [LadderEncoder([neurons[i - 1], neurons[i], z_dim[i - 1]]) for i in range(1, len(neurons))]\n",
        "\n",
        "        e = encoder_layers[-1]\n",
        "        encoder_layers[-1] = LadderEncoder([e.in_features + y_dim, e.out_features, e.z_dim])\n",
        "\n",
        "        decoder_layers = [LadderDecoder([z_dim[i - 1], h_dim[i - 1], z_dim[i]]) for i in range(1, len(h_dim))][::-1]\n",
        "\n",
        "        self.classifier = Classifier([x_dim, h_dim[0], y_dim])\n",
        "\n",
        "        self.encoder = nn.ModuleList(encoder_layers)\n",
        "        self.decoder = nn.ModuleList(decoder_layers)\n",
        "        self.reconstruction = Decoder([z_dim[0]+y_dim, h_dim, x_dim])\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                init.xavier_normal(m.weight.data)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # Gather latent representation\n",
        "        # from encoders along with final z.\n",
        "        latents = []\n",
        "        for i, encoder in enumerate(self.encoder):\n",
        "            if i == len(self.encoder)-1:\n",
        "                x, (z, mu, log_var) = encoder(torch.cat([x, y], dim=1))\n",
        "            else:\n",
        "                x, (z, mu, log_var) = encoder(x)\n",
        "            latents.append((mu, log_var))\n",
        "\n",
        "        latents = list(reversed(latents))\n",
        "\n",
        "        self.kl_divergence = 0\n",
        "        for i, decoder in enumerate([-1, *self.decoder]):\n",
        "            # If at top, encoder == decoder,\n",
        "            # use prior for KL.\n",
        "            l_mu, l_log_var = latents[i]\n",
        "            if i == 0:\n",
        "                self.kl_divergence += self._kld(z, (l_mu, l_log_var))\n",
        "\n",
        "            # Perform downword merge of information.\n",
        "            else:\n",
        "                z, kl = decoder(z, l_mu, l_log_var)\n",
        "                self.kl_divergence += self._kld(*kl)\n",
        "\n",
        "        x_mu = self.reconstruction(torch.cat([z, y], dim=1))\n",
        "        return x_mu\n",
        "\n",
        "    def sample(self, z, y):\n",
        "        for i, decoder in enumerate(self.decoder):\n",
        "            z = decoder(z)\n",
        "        return self.reconstruction(torch.cat([z, y], dim=1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGLtoyNPOfAd",
        "colab_type": "text"
      },
      "source": [
        "# Deep Generative Model\n",
        "\n",
        "In this notebook we show how you can use the deep \"generative model for semi-supervised learning\" as presented in [[Kingma 2014]](https://arxiv.org/abs/1406.5298). The paper posits three different models, though we are just interested in two of these: the M2 model and the M1+M2 model.\n",
        "\n",
        "The M1 model is just a variational autoencoder, so we refer to the previous notebook for more information on this. The M2 model however is an extension to the VAE to include label information for a semi-supervised objective. The structure is shown below (left: inference model, right: generative model).\n",
        "\n",
        "<img src=\"../images/dgm.png\" width=\"400px\"/>\n",
        "\n",
        "The point of the generative model is to seperate the partially observed label information $y$ from the latent variable $z$ in order to learn a representation that seperates these two variables. We can use this model for semi-supervised learning as the inference model must also infer the label from the data $x$ along with the latent variable $z$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHQTQaVVOfAe",
        "colab_type": "code",
        "outputId": "ac3ccc43-b128-4279-deb4-e898914c7002",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        }
      },
      "source": [
        "y_dim = 10\n",
        "z_dim = 32\n",
        "h_dim = [256, 128]\n",
        "\n",
        "model = DeepGenerativeModel([784, y_dim, z_dim, h_dim])\n",
        "model"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:316: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:585: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeepGenerativeModel(\n",
              "  (encoder): Encoder(\n",
              "    (hidden): ModuleList(\n",
              "      (0): Linear(in_features=794, out_features=256, bias=True)\n",
              "      (1): Linear(in_features=256, out_features=128, bias=True)\n",
              "    )\n",
              "    (sample): GaussianSample(\n",
              "      (mu): Linear(in_features=128, out_features=32, bias=True)\n",
              "      (log_var): Linear(in_features=128, out_features=32, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (hidden): ModuleList(\n",
              "      (0): Linear(in_features=42, out_features=128, bias=True)\n",
              "      (1): Linear(in_features=128, out_features=256, bias=True)\n",
              "    )\n",
              "    (reconstruction): Linear(in_features=256, out_features=784, bias=True)\n",
              "    (output_activation): Sigmoid()\n",
              "  )\n",
              "  (classifier): Classifier(\n",
              "    (dense): Linear(in_features=784, out_features=256, bias=True)\n",
              "    (logits): Linear(in_features=256, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeihgXIPOfAk",
        "colab_type": "code",
        "outputId": "da2ba00d-0aa5-4af4-bf16-8119c9cb9baa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(model.encoder.hidden[0])\n",
        "print(model.decoder.hidden[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear(in_features=794, out_features=256, bias=True)\n",
            "Linear(in_features=42, out_features=128, bias=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho_X648iOfAo",
        "colab_type": "text"
      },
      "source": [
        "Notice how theres now a classifier associated with model. This classifier will just be a simple model that takes the size of the first layer encoder network. We also have a larger input space on both the encoder and decoder to make room for label information, in this case 10 labels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4zqQSVVOfAp",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "Recall the ELBO from the VAE formulation, we want to construct a similar ELBO when we include labelled data $y$. In the case that we have labels, the ELBO has a simple formulation that is similar to the one for the VAE. The difference here is that we must also have a prior over labels $p(y)$, which we choose to be uniform over the different classes.\n",
        "\n",
        "\\begin{align}\n",
        "\\log p(x, y) &= \\log \\int q(z|x, y) \\frac{p(x, y, z)}{q(z|xy)} \\ dz \\geq \\int q(z|x, y) \\log \\frac{p(x, y, z)}{q(z|xy)} \\ dz\\\\\n",
        "&= \\int q(z|x, y) [ \\log p(x|z,y) + \\log p(y) ] \\ dz + \\int q(z|x, y) \\log \\frac{p(z)}{q(z|xy)} \\ dz\\\\\n",
        "&= \\mathbb{E}_{q(z|x, y)} [ \\log p(x|z,y) + \\log p(y) ] - KL(p(z)||q(z|xy)) = - \\mathcal{L}(x, y)\n",
        "\\end{align}\n",
        "\n",
        "In the case when the labels are not observed, we can instead integrate over all of the labels to achieve the same effect.\n",
        "\n",
        "\\begin{align}\n",
        "\\log p(x) &= \\log \\sum_{y} \\int q(z,y|x) \\frac{p(x, y, z)}{q(z,y|x)} \\ dz \\geq \\sum_{y} q(y|x) \\int q(z|x, y) \\log \\frac{p(x, y, z)}{q(z,y|x)} \\ dz\\\\\n",
        "&= \\sum_{y} q(y|x) \\int q(z|x, y) \\log \\frac{p(x, y, z)}{q(z,y|x)} \\ dz + \\sum_{y} q(y|x) \\log q(y|x) \\int q(z|x, y) \\ dz\\\\\n",
        "&= \\sum_{y} q(y|x) (- \\mathcal{L}(x,y)) + \\mathcal{H}(q(y|x)) = - \\mathcal{U}(x)\n",
        "\\end{align}\n",
        "\n",
        "Notice how in both cases we need to compute the labelled bound, but in the unlabelled case we need to do it $n$ times where $n$ is the number of classes. In this model, we do not learn directly from the labelled class, as there is no cross entropy term between $y$ and our model output $q(y|x)$. We therefore add an auxiliary loss to arrive at the final loss objective.\n",
        "\n",
        "$$\\mathcal{J}^{\\alpha} = \\sum_{(x_l, y_l)}\\mathcal{L}(x_l, y_l) + \\alpha \\cdot \\mathbb{E}_{x_l, y_l}[- \\log q(y_l|x_l)] + \\sum_{(x_u)}\\mathcal{U}(x_u)$$\n",
        "\n",
        "Where $l, u$ denotes labelled and unlabelled data respectively and $\\alpha$ is a hyperparameter that denotes the reliance of labelled data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hg3lkzTRJkr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def enumerate_discrete(x, y_dim):\n",
        "    \"\"\"\n",
        "    Generates a `torch.Tensor` of size batch_size x n_labels of\n",
        "    the given label.\n",
        "\n",
        "    Example: generate_label(2, 1, 3) #=> torch.Tensor([[0, 1, 0],\n",
        "                                                       [0, 1, 0]])\n",
        "    :param x: tensor with batch size to mimic\n",
        "    :param y_dim: number of total labels\n",
        "    :return variable\n",
        "    \"\"\"\n",
        "    def batch(batch_size, label):\n",
        "        labels = (torch.ones(batch_size, 1) * label).type(torch.LongTensor)\n",
        "        y = torch.zeros((batch_size, y_dim))\n",
        "        y.scatter_(1, labels, 1)\n",
        "        return y.type(torch.LongTensor)\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "    generated = torch.cat([batch(batch_size, i) for i in range(y_dim)])\n",
        "\n",
        "    if x.is_cuda:\n",
        "        generated = generated.cuda()\n",
        "\n",
        "    return Variable(generated.float())\n",
        "\n",
        "\n",
        "def onehot(k):\n",
        "    \"\"\"\n",
        "    Converts a number to its one-hot or 1-of-k representation\n",
        "    vector.\n",
        "    :param k: (int) length of vector\n",
        "    :return: onehot function\n",
        "    \"\"\"\n",
        "    def encode(label):\n",
        "        y = torch.zeros(k)\n",
        "        if label < k:\n",
        "            y[label] = 1\n",
        "        return y\n",
        "    return encode\n",
        "\n",
        "\n",
        "def log_sum_exp(tensor, dim=-1, sum_op=torch.sum):\n",
        "    \"\"\"\n",
        "    Uses the LogSumExp (LSE) as an approximation for the sum in a log-domain.\n",
        "    :param tensor: Tensor to compute LSE over\n",
        "    :param dim: dimension to perform operation over\n",
        "    :param sum_op: reductive operation to be applied, e.g. torch.sum or torch.mean\n",
        "    :return: LSE\n",
        "    \"\"\"\n",
        "    max, _ = torch.max(tensor, dim=dim, keepdim=True)\n",
        "    return torch.log(sum_op(torch.exp(tensor - max), dim=dim, keepdim=True) + 1e-8) + max"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "26PinGUROfAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import sys\n",
        "from urllib import request\n",
        "from torch.utils.data import Dataset\n",
        "n_labels = 10\n",
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "\n",
        "def get_mnist(location=\"./\", batch_size=64, labels_per_class=100):\n",
        "    from functools import reduce\n",
        "    from operator import __or__\n",
        "    from torch.utils.data.sampler import SubsetRandomSampler\n",
        "    from torchvision.datasets import MNIST\n",
        "    import torchvision.transforms as transforms\n",
        "\n",
        "    flatten_bernoulli = lambda x: transforms.ToTensor()(x).view(-1).bernoulli()\n",
        "\n",
        "    mnist_train = MNIST(location, train=True, download=True,\n",
        "                        transform=flatten_bernoulli, target_transform=onehot(n_labels))\n",
        "    mnist_valid = MNIST(location, train=False, download=True,\n",
        "                        transform=flatten_bernoulli, target_transform=onehot(n_labels))\n",
        "\n",
        "    def get_sampler(labels, n=None):\n",
        "        # Only choose digits in n_labels\n",
        "        (indices,) = np.where(reduce(__or__, [labels == i for i in np.arange(n_labels)]))\n",
        "\n",
        "        # Ensure uniform distribution of labels\n",
        "        np.random.shuffle(indices)\n",
        "        indices = np.hstack([list(filter(lambda idx: labels[idx] == i, indices))[:n] for i in range(n_labels)])\n",
        "\n",
        "        indices = torch.from_numpy(indices)\n",
        "        sampler = SubsetRandomSampler(indices)\n",
        "        return sampler\n",
        "\n",
        "    # Dataloaders for MNIST\n",
        "    labelled = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, num_workers=2, pin_memory=cuda,\n",
        "                                           sampler=get_sampler(mnist_train.train_labels.numpy(), labels_per_class))\n",
        "    unlabelled = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, num_workers=2, pin_memory=cuda,\n",
        "                                             sampler=get_sampler(mnist_train.train_labels.numpy()))\n",
        "    validation = torch.utils.data.DataLoader(mnist_valid, batch_size=batch_size, num_workers=2, pin_memory=cuda,\n",
        "                                             sampler=get_sampler(mnist_valid.test_labels.numpy()))\n",
        "\n",
        "    return labelled, unlabelled, validation\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A7lNj4cRBKP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import repeat\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def log_standard_gaussian(x):\n",
        "    \"\"\"\n",
        "    Evaluates the log pdf of a standard normal distribution at x.\n",
        "\n",
        "    :param x: point to evaluate\n",
        "    :return: log N(x|0,I)\n",
        "    \"\"\"\n",
        "    return torch.sum(-0.5 * math.log(2 * math.pi) - x ** 2 / 2, dim=-1)\n",
        "\n",
        "def log_gaussian(x, mu, log_var):\n",
        "    \"\"\"\n",
        "    Returns the log pdf of a normal distribution parametrised\n",
        "    by mu and log_var evaluated at x.\n",
        "\n",
        "    :param x: point to evaluate\n",
        "    :param mu: mean of distribution\n",
        "    :param log_var: log variance of distribution\n",
        "    :return: log N(x|µ,σ)\n",
        "    \"\"\"\n",
        "    log_pdf = - 0.5 * math.log(2 * math.pi) - log_var / 2 - (x - mu)**2 / (2 * torch.exp(log_var))\n",
        "    return torch.sum(log_pdf, dim=-1)\n",
        "\n",
        "def log_standard_categorical(p):\n",
        "    \"\"\"\n",
        "    Calculates the cross entropy between a (one-hot) categorical vector\n",
        "    and a standard (uniform) categorical distribution.\n",
        "\n",
        "    :param p: one-hot categorical distribution\n",
        "    :return: H(p, u)\n",
        "    \"\"\"\n",
        "    # Uniform prior over y\n",
        "    prior = F.softmax(torch.ones_like(p), dim=1)\n",
        "    prior.requires_grad = False\n",
        "\n",
        "    cross_entropy = -torch.sum(p * torch.log(prior + 1e-8), dim=1)\n",
        "\n",
        "    return cross_entropy\n",
        "\n",
        "class ImportanceWeightedSampler(object):\n",
        "    \"\"\"\n",
        "    Importance weighted sampler [Burda 2015] to\n",
        "    be used in conjunction with SVI.\n",
        "    \"\"\"\n",
        "    def __init__(self, mc=1, iw=1):\n",
        "        \"\"\"\n",
        "        Initialise a new sampler.\n",
        "        :param mc: number of Monte Carlo samples\n",
        "        :param iw: number of Importance Weighted samples\n",
        "        \"\"\"\n",
        "        self.mc = mc\n",
        "        self.iw = iw\n",
        "\n",
        "    def resample(self, x):\n",
        "        return x.repeat(self.mc * self.iw, 1)\n",
        "\n",
        "    def __call__(self, elbo):\n",
        "        elbo = elbo.view(self.mc, self.iw, -1)\n",
        "        elbo = torch.mean(log_sum_exp(elbo, dim=1, sum_op=torch.mean), dim=0)\n",
        "        return elbo.view(-1)\n",
        "\n",
        "class SVI(nn.Module):\n",
        "    \"\"\"\n",
        "    Stochastic variational inference (SVI).\n",
        "    \"\"\"\n",
        "    base_sampler = ImportanceWeightedSampler(mc=1, iw=1)\n",
        "    def __init__(self, model, likelihood=F.binary_cross_entropy, beta=repeat(1), sampler=base_sampler):\n",
        "        \"\"\"\n",
        "        Initialises a new SVI optimizer for semi-\n",
        "        supervised learning.\n",
        "        :param model: semi-supervised model to evaluate\n",
        "        :param likelihood: p(x|y,z) for example BCE or MSE\n",
        "        :param sampler: sampler for x and y, e.g. for Monte Carlo\n",
        "        :param beta: warm-up/scaling of KL-term\n",
        "        \"\"\"\n",
        "        super(SVI, self).__init__()\n",
        "        self.model = model\n",
        "        self.likelihood = likelihood\n",
        "        self.sampler = sampler\n",
        "        self.beta = beta\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        is_labelled = False if y is None else True\n",
        "\n",
        "        # Prepare for sampling\n",
        "        xs, ys = (x, y)\n",
        "\n",
        "        # Enumerate choices of label\n",
        "        if not is_labelled:\n",
        "            ys = enumerate_discrete(xs, self.model.y_dim)\n",
        "            xs = xs.repeat(self.model.y_dim, 1)\n",
        "\n",
        "        # Increase sampling dimension\n",
        "        xs = self.sampler.resample(xs)\n",
        "        ys = self.sampler.resample(ys)\n",
        "\n",
        "        reconstruction = self.model(xs, ys)\n",
        "\n",
        "        # p(x|y,z)\n",
        "        likelihood = -self.likelihood(reconstruction, xs)\n",
        "\n",
        "        # p(y)\n",
        "        prior = -log_standard_categorical(ys)\n",
        "\n",
        "        # Equivalent to -L(x, y)\n",
        "        elbo = likelihood + prior - next(self.beta) * self.model.kl_divergence\n",
        "        L = self.sampler(elbo)\n",
        "\n",
        "        if is_labelled:\n",
        "            return torch.mean(L)\n",
        "\n",
        "        logits = self.model.classify(x)\n",
        "\n",
        "        L = L.view_as(logits.t()).t()\n",
        "\n",
        "        # Calculate entropy H(q(y|x)) and sum over all labels\n",
        "        H = -torch.sum(torch.mul(logits, torch.log(logits + 1e-8)), dim=-1)\n",
        "        L = torch.sum(torch.mul(logits, L), dim=-1)\n",
        "\n",
        "        # Equivalent to -U(x)\n",
        "        U = L + H\n",
        "        return torch.mean(U)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcc0Lx-6OfAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import cycle\n",
        "\n",
        "# You can use importance weighted samples [Burda, 2015] to get a better estimate\n",
        "# on the log-likelihood.\n",
        "sampler = ImportanceWeightedSampler(mc=1, iw=1)\n",
        "\n",
        "if cuda: model = model.cuda()\n",
        "elbo = SVI(model, likelihood=binary_cross_entropy, sampler=sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kZsh63uOfAy",
        "colab_type": "text"
      },
      "source": [
        "The library is conventially packed with the `SVI` method that does all of the work of calculating the lower bound for both labelled and unlabelled data depending on whether the label is given. It also manages to perform the enumeration of all the labels.\n",
        "\n",
        "Remember that the labels have to be in a *one-hot encoded* format in order to work with SVI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hisvrr8y4JXc",
        "colab_type": "text"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgVZAt_kOfA5",
        "colab_type": "code",
        "outputId": "4447eeec-e942-4762-ae64-44bc17e27fe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Only use 10 labelled examples per class\n",
        "# The rest of the data is unlabelled.\n",
        "labelled, unlabelled, validation = get_mnist(location=\"./\", batch_size=64, labels_per_class=10)\n",
        "alpha = 0.1 * len(unlabelled) / len(labelled)\n",
        "\n",
        "def binary_cross_entropy(r, x):\n",
        "    return -torch.sum(x * torch.log(r + 1e-8) + (1 - x) * torch.log(1 - r + 1e-8), dim=-1)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.999))\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss, accuracy = (0, 0)\n",
        "    for (x, y), (u, _) in zip(iter(labelled), iter(unlabelled)):\n",
        "        # Wrap in variables\n",
        "        x, y, u = Variable(x), Variable(y), Variable(u)\n",
        "\n",
        "        if cuda:\n",
        "            # They need to be on the same device and be synchronized.\n",
        "            x, y = x.cuda(device=0), y.cuda(device=0)\n",
        "            u = u.cuda(device=0)\n",
        "\n",
        "        L = -elbo(x, y)\n",
        "        U = -elbo(u)\n",
        "\n",
        "        # Add auxiliary classification loss q(y|x)\n",
        "        logits = model.classify(x)\n",
        "        \n",
        "        # Regular cross entropy\n",
        "        classication_loss = torch.sum(y * torch.log(logits + 1e-8), dim=1).mean()\n",
        "\n",
        "        J_alpha = L - alpha * classication_loss + U\n",
        "\n",
        "        J_alpha.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_loss += torch.Tensor.item(J_alpha.data)\n",
        "        accuracy += torch.mean((torch.max(logits, 1)[1].data == torch.max(y, 1)[1].data).float())\n",
        "        \n",
        "    if epoch % 1 == 0:\n",
        "        model.eval()\n",
        "        m = len(unlabelled)\n",
        "        print(\"training\")\n",
        "        print(m)\n",
        "        print(accuracy)\n",
        "        print(\"Epoch: {}\".format(epoch))\n",
        "        print(\"[Train]\\t\\t J_a: {:.2f}, accuracy: {:.2f}\".format(total_loss / m, accuracy / m))\n",
        "\n",
        "        total_loss, accuracy = (0, 0)\n",
        "        for x, y in validation:\n",
        "            x, y = Variable(x), Variable(y)\n",
        "\n",
        "            if cuda:\n",
        "                x, y = x.cuda(device=0), y.cuda(device=0)\n",
        "\n",
        "            L = -elbo(x, y)\n",
        "            U = -elbo(x)\n",
        "\n",
        "            logits = model.classify(x)\n",
        "            classication_loss = -torch.sum(y * torch.log(logits + 1e-8), dim=1).mean()\n",
        "\n",
        "            J_alpha = L + alpha * classication_loss + U\n",
        "\n",
        "            total_loss += torch.Tensor.item(J_alpha.data)\n",
        "\n",
        "            _, pred_idx = torch.max(logits, 1)\n",
        "            _, lab_idx = torch.max(y, 1)\n",
        "            accuracy += torch.mean((torch.max(logits, 1)[1].data == torch.max(y, 1)[1].data).float())\n",
        "\n",
        "        m = len(validation)\n",
        "        print(\"validation\")\n",
        "        print(m)\n",
        "        print(accuracy)\n",
        "        print(\"[Validation]\\t J_a: {:.2f}, accuracy: {:.2f}\".format(total_loss / m, accuracy / m))\n",
        "   "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training\n",
            "938\n",
            "tensor(0.2674)\n",
            "Epoch: 0\n",
            "[Train]\t\t J_a: 2.60, accuracy: 0.00\n",
            "validation\n",
            "157\n",
            "tensor(28.8906)\n",
            "[Validation]\t J_a: 1198.44, accuracy: 0.18\n",
            "training\n",
            "938\n",
            "tensor(0.4219)\n",
            "Epoch: 1\n",
            "[Train]\t\t J_a: 2.54, accuracy: 0.00\n",
            "validation\n",
            "157\n",
            "tensor(35.4219)\n",
            "[Validation]\t J_a: 1172.54, accuracy: 0.23\n",
            "training\n",
            "938\n",
            "tensor(0.6146)\n",
            "Epoch: 2\n",
            "[Train]\t\t J_a: 2.48, accuracy: 0.00\n",
            "validation\n",
            "157\n",
            "tensor(42.9688)\n",
            "[Validation]\t J_a: 1147.77, accuracy: 0.27\n",
            "training\n",
            "938\n",
            "tensor(0.7292)\n",
            "Epoch: 3\n",
            "[Train]\t\t J_a: 2.42, accuracy: 0.00\n",
            "validation\n",
            "157\n",
            "tensor(52.5781)\n",
            "[Validation]\t J_a: 1122.95, accuracy: 0.33\n",
            "training\n",
            "938\n",
            "tensor(1.0208)\n",
            "Epoch: 4\n",
            "[Train]\t\t J_a: 2.37, accuracy: 0.00\n",
            "validation\n",
            "157\n",
            "tensor(60.4375)\n",
            "[Validation]\t J_a: 1097.10, accuracy: 0.38\n",
            "training\n",
            "938\n",
            "tensor(1.0694)\n",
            "Epoch: 5\n",
            "[Train]\t\t J_a: 2.31, accuracy: 0.00\n",
            "validation\n",
            "157\n",
            "tensor(67.9688)\n",
            "[Validation]\t J_a: 1069.50, accuracy: 0.43\n",
            "training\n",
            "938\n",
            "tensor(1.3194)\n",
            "Epoch: 6\n",
            "[Train]\t\t J_a: 2.24, accuracy: 0.00\n",
            "validation\n",
            "157\n",
            "tensor(75.0781)\n",
            "[Validation]\t J_a: 1039.43, accuracy: 0.48\n",
            "training\n",
            "938\n",
            "tensor(1.4618)\n",
            "Epoch: 7\n",
            "[Train]\t\t J_a: 2.17, accuracy: 0.00\n",
            "validation\n",
            "157\n",
            "tensor(80.2031)\n",
            "[Validation]\t J_a: 1006.27, accuracy: 0.51\n",
            "training\n",
            "938\n",
            "tensor(1.3889)\n",
            "Epoch: 8\n",
            "[Train]\t\t J_a: 2.10, accuracy: 0.00\n",
            "validation\n",
            "157\n",
            "tensor(83.3594)\n",
            "[Validation]\t J_a: 969.46, accuracy: 0.53\n",
            "training\n",
            "938\n",
            "tensor(1.5035)\n",
            "Epoch: 9\n",
            "[Train]\t\t J_a: 2.02, accuracy: 0.00\n",
            "validation\n",
            "157\n",
            "tensor(87.2188)\n",
            "[Validation]\t J_a: 928.52, accuracy: 0.56\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}