{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "final_deep_generative_model_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [
          0
        ],
        "id": "wEJXnNF8OfAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "from itertools import repeat, cycle\n",
        "from functools import reduce\n",
        "from operator import __or__\n",
        "\n",
        "#cuda = torch.cuda.is_available()\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "#from urllib import request\n",
        "\n",
        "# code modified using examples from https://github.com/wohlert/semi-supervised-pytorch\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY-jH9ICOjqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_standard_gaussian(x):\n",
        "    \"\"\"\n",
        "    Evaluates the log pdf of a standard normal distribution at x.\n",
        "\n",
        "    :param x: point to evaluate\n",
        "    :return: log N(x|0,I)\n",
        "    \"\"\"\n",
        "    return torch.sum(-0.5 * math.log(2 * math.pi) - x ** 2 / 2, dim=-1)\n",
        "\n",
        "\n",
        "def log_gaussian(x, mu, log_var):\n",
        "    \"\"\"\n",
        "    Returns the log pdf of a normal distribution parametrised\n",
        "    by mu and log_var evaluated at x.\n",
        "\n",
        "    :param x: point to evaluate\n",
        "    :param mu: mean of distribution\n",
        "    :param log_var: log variance of distribution\n",
        "    :return: log N(x|µ,σ)\n",
        "    \"\"\"\n",
        "    log_pdf = - 0.5 * math.log(2 * math.pi) - log_var / 2 - (x - mu)**2 / (2 * torch.exp(log_var))\n",
        "    return torch.sum(log_pdf, dim=-1)\n",
        "\n",
        "\n",
        "def log_standard_categorical(p):\n",
        "    \"\"\"\n",
        "    Calculates the cross entropy between a (one-hot) categorical vector\n",
        "    and a standard (uniform) categorical distribution.\n",
        "\n",
        "    :param p: one-hot categorical distribution\n",
        "    :return: H(p, u)\n",
        "    \"\"\"\n",
        "    # Uniform prior over y\n",
        "    prior = F.softmax(torch.ones_like(p), dim=1)\n",
        "    prior.requires_grad = False\n",
        "\n",
        "    cross_entropy = -torch.sum(p * torch.log(prior + 1e-8), dim=1)\n",
        "\n",
        "    return cross_entropy\n",
        "\n",
        "def enumerate_discrete(x, y_dim):\n",
        "    \"\"\"\n",
        "    Generates a `torch.Tensor` of size batch_size x n_labels of\n",
        "    the given label.\n",
        "    :param x: tensor with batch size to mimic\n",
        "    :param y_dim: number of total labels\n",
        "    :return variable\n",
        "    \"\"\"\n",
        "    def batch(batch_size, label):\n",
        "        labels = (torch.ones(batch_size, 1) * label).type(torch.LongTensor)\n",
        "        y = torch.zeros((batch_size, y_dim))\n",
        "        y.scatter_(1, labels, 1)\n",
        "        return y.type(torch.LongTensor)\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "    generated = torch.cat([batch(batch_size, i) for i in range(y_dim)])\n",
        "\n",
        "    return Variable(generated.float())\n",
        "\n",
        "\n",
        "def onehot(k):\n",
        "    \"\"\"\n",
        "    Converts a number to its one-hot or 1-of-k representation\n",
        "    vector.\n",
        "    :param k: (int) length of vector\n",
        "    :return: onehot function\n",
        "    \"\"\"\n",
        "    def encode(label):\n",
        "        y = torch.zeros(k)\n",
        "        if label < k:\n",
        "            y[label] = 1\n",
        "        return y\n",
        "    return encode\n",
        "\n",
        "\n",
        "def log_sum_exp(tensor, dim=-1, sum_op=torch.sum):\n",
        "    \"\"\"\n",
        "    Uses the LogSumExp (LSE) as an approximation for the sum in a log-domain.\n",
        "    :param tensor: Tensor to compute LSE over\n",
        "    :param dim: dimension to perform operation over\n",
        "    :param sum_op: reductive operation to be applied, e.g. torch.sum or torch.mean\n",
        "    :return: LSE\n",
        "    \"\"\"\n",
        "    max, _ = torch.max(tensor, dim=dim, keepdim=True)\n",
        "    return torch.log(sum_op(torch.exp(tensor - max), dim=dim, keepdim=True) + 1e-8) + max\n",
        "\n",
        "def binary_cross_entropy(r, x):\n",
        "    return -torch.sum(x * torch.log(r + 1e-8) + (1 - x) * torch.log(1 - r + 1e-8), dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxSLep22i8Ex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Stochastic(nn.Module):\n",
        "    \"\"\"\n",
        "    Base stochastic layer that uses the\n",
        "    reparametrization trick [Kingma 2013]\n",
        "    to draw a sample from a distribution\n",
        "    parametrised by mu and log_var.\n",
        "    \"\"\"\n",
        "    def reparametrize(self, mu, log_var):\n",
        "        epsilon = Variable(torch.randn(mu.size()), requires_grad=False)\n",
        "\n",
        "        if mu.is_cuda:\n",
        "            epsilon = epsilon.cuda()\n",
        "        \n",
        "        # log_std = 0.5 * log_var\n",
        "        # std = exp(log_std)\n",
        "        std = log_var.mul(0.5).exp_()\n",
        "\n",
        "        # z = std * epsilon + mu\n",
        "        z = mu.addcmul(std, epsilon)\n",
        "        return z\n",
        "\n",
        "class GaussianSample(Stochastic):\n",
        "    \"\"\"\n",
        "    Layer that represents a sample from a\n",
        "    Gaussian distribution.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(GaussianSample, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        self.mu = nn.Linear(in_features, out_features)\n",
        "        self.log_var = nn.Linear(in_features, out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu = self.mu(x)\n",
        "        log_var = F.softplus(self.log_var(x))\n",
        "\n",
        "        return self.reparametrize(mu, log_var), mu, log_var\n",
        "\n",
        "class GaussianMerge(GaussianSample):\n",
        "    \"\"\"\n",
        "    Precision weighted merging of two Gaussian\n",
        "    distributions.\n",
        "    Merges information from z into the given\n",
        "    mean and log variance and produces\n",
        "    a sample from this new distribution.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(GaussianMerge, self).__init__(in_features, out_features)\n",
        "\n",
        "    def forward(self, z, mu1, log_var1):\n",
        "        # Calculate precision of each distribution\n",
        "        # (inverse variance)\n",
        "        mu2 = self.mu(z)\n",
        "        log_var2 = F.softplus(self.log_var(z))\n",
        "        precision1, precision2 = (1/torch.exp(log_var1), 1/torch.exp(log_var2))\n",
        "\n",
        "        # Merge distributions into a single new\n",
        "        # distribution\n",
        "        mu = ((mu1 * precision1) + (mu2 * precision2)) / (precision1 + precision2)\n",
        "\n",
        "        var = 1 / (precision1 + precision2)\n",
        "        log_var = torch.log(var + 1e-8)\n",
        "\n",
        "        return self.reparametrize(mu, log_var), mu, log_var\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, dims, sample_layer=GaussianSample):\n",
        "        \"\"\"\n",
        "        Inference network\n",
        "\n",
        "        Attempts to infer the probability distribution\n",
        "        p(z|x) from the data by fitting a variational\n",
        "        distribution q_φ(z|x). Returns the two parameters\n",
        "        of the distribution (µ, log σ²).\n",
        "\n",
        "        :param dims: dimensions of the networks\n",
        "           given by the number of neurons on the form\n",
        "           [input_dim, [hidden_dims], latent_dim].\n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        [x_dim, h_dim, z_dim] = dims\n",
        "        neurons = [x_dim, *h_dim]\n",
        "        linear_layers = [nn.Linear(neurons[i-1], neurons[i]) for i in range(1, len(neurons))]\n",
        "\n",
        "        self.hidden = nn.ModuleList(linear_layers)\n",
        "        self.sample = sample_layer(h_dim[-1], z_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.hidden:\n",
        "            x = F.relu(layer(x))\n",
        "        return self.sample(x)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        \"\"\"\n",
        "        Generative network\n",
        "\n",
        "        Generates samples from the original distribution\n",
        "        p(x) by transforming a latent representation, e.g.\n",
        "        by finding p_θ(x|z).\n",
        "\n",
        "        :param dims: dimensions of the networks\n",
        "            given by the number of neurons on the form\n",
        "            [latent_dim, [hidden_dims], input_dim].\n",
        "        \"\"\"\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        [z_dim, h_dim, x_dim] = dims\n",
        "\n",
        "        neurons = [z_dim, *h_dim]\n",
        "        linear_layers = [nn.Linear(neurons[i-1], neurons[i]) for i in range(1, len(neurons))]\n",
        "        self.hidden = nn.ModuleList(linear_layers)\n",
        "\n",
        "        self.reconstruction = nn.Linear(h_dim[-1], x_dim)\n",
        "\n",
        "        self.output_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.hidden:\n",
        "            x = F.relu(layer(x))\n",
        "        return self.output_activation(self.reconstruction(x))\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    # need to change this so conforms to other algorithms layer definition\n",
        "    def __init__(self, dims):\n",
        "        \"\"\"\n",
        "        Single hidden layer classifier\n",
        "        with softmax output.\n",
        "        \"\"\"\n",
        "        super(Classifier, self).__init__()\n",
        "        [x_dim, h_dim, y_dim] = dims\n",
        "        self.dense = nn.Linear(x_dim, h_dim)\n",
        "        self.logits = nn.Linear(h_dim, y_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.dense(x))\n",
        "        x = F.softmax(self.logits(x), dim=-1)\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnfEhO7DqUsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# models\n",
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        \"\"\"\n",
        "        Variational Autoencoder [Kingma 2013] model\n",
        "        consisting of an encoder/decoder pair for which\n",
        "        a variational distribution is fitted to the\n",
        "        encoder. Also known as the M1 model in [Kingma 2014].\n",
        "\n",
        "        :param dims: x, z and hidden dimensions of the networks\n",
        "        \"\"\"\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "\n",
        "        [x_dim, z_dim, h_dim] = dims\n",
        "        self.z_dim = z_dim\n",
        "        self.flow = None\n",
        "\n",
        "        self.encoder = Encoder([x_dim, h_dim, z_dim])\n",
        "        self.decoder = Decoder([z_dim, list(reversed(h_dim)), x_dim])\n",
        "        self.kl_divergence = 0\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                init.xavier_normal(m.weight.data)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "\n",
        "    def _kld(self, z, q_param, p_param=None):\n",
        "        \"\"\"\n",
        "        Computes the KL-divergence of\n",
        "        some element z.\n",
        "\n",
        "        KL(q||p) = -∫ q(z) log [ p(z) / q(z) ]\n",
        "                 = -E[log p(z) - log q(z)]\n",
        "\n",
        "        :param z: sample from q-distribuion\n",
        "        :param q_param: (mu, log_var) of the q-distribution\n",
        "        :param p_param: (mu, log_var) of the p-distribution\n",
        "        :return: KL(q||p)\n",
        "        \"\"\"\n",
        "        (mu, log_var) = q_param\n",
        "\n",
        "        if self.flow is not None:\n",
        "            f_z, log_det_z = self.flow(z)\n",
        "            qz = log_gaussian(z, mu, log_var) - sum(log_det_z)\n",
        "            z = f_z\n",
        "        else:\n",
        "            qz = log_gaussian(z, mu, log_var)\n",
        "\n",
        "        if p_param is None:\n",
        "            pz = log_standard_gaussian(z)\n",
        "        else:\n",
        "            (mu, log_var) = p_param\n",
        "            pz = log_gaussian(z, mu, log_var)\n",
        "\n",
        "        kl = qz - pz\n",
        "\n",
        "        return kl\n",
        "\n",
        "    def add_flow(self, flow):\n",
        "        self.flow = flow\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        \"\"\"\n",
        "        Runs a data point through the model in order\n",
        "        to provide its reconstruction and q distribution\n",
        "        parameters.\n",
        "\n",
        "        :param x: input data\n",
        "        :return: reconstructed input\n",
        "        \"\"\"\n",
        "        z, z_mu, z_log_var = self.encoder(x)\n",
        "\n",
        "        self.kl_divergence = self._kld(z, (z_mu, z_log_var))\n",
        "\n",
        "        x_mu = self.decoder(z)\n",
        "\n",
        "        return x_mu\n",
        "\n",
        "    def sample(self, z):\n",
        "        \"\"\"\n",
        "        Given z ~ N(0, I) generates a sample from\n",
        "        the learned distribution based on p_θ(x|z).\n",
        "        :param z: (torch.autograd.Variable) Random normal variable\n",
        "        :return: (torch.autograd.Variable) generated sample\n",
        "        \"\"\"\n",
        "        return self.decoder(z)\n",
        "\n",
        "\n",
        "class DeepGenerativeModel(VariationalAutoencoder):\n",
        "    def __init__(self, dims):\n",
        "        \"\"\"\n",
        "        M2 code replication from the paper\n",
        "        'Semi-Supervised Learning with Deep Generative Models'\n",
        "        (Kingma 2014) in PyTorch.\n",
        "\n",
        "        The \"Generative semi-supervised model\" is a probabilistic\n",
        "        model that incorporates label information in both\n",
        "        inference and generation.\n",
        "\n",
        "        Initialise a new generative model\n",
        "        :param dims: dimensions of x, y, z and hidden layers.\n",
        "        \"\"\"\n",
        "        [x_dim, self.y_dim, z_dim, h_dim] = dims\n",
        "        super(DeepGenerativeModel, self).__init__([x_dim, z_dim, h_dim])\n",
        "\n",
        "        self.encoder = Encoder([x_dim + self.y_dim, h_dim, z_dim])\n",
        "        self.decoder = Decoder([z_dim + self.y_dim, list(reversed(h_dim)), x_dim])\n",
        "        self.classifier = Classifier([x_dim, h_dim[0], self.y_dim])\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                init.xavier_normal(m.weight.data)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # Add label and data and generate latent variable\n",
        "        z, z_mu, z_log_var = self.encoder(torch.cat([x, y], dim=1))\n",
        "\n",
        "        self.kl_divergence = self._kld(z, (z_mu, z_log_var))\n",
        "\n",
        "        # Reconstruct data point from latent data and label\n",
        "        x_mu = self.decoder(torch.cat([z, y], dim=1))\n",
        "\n",
        "        return x_mu\n",
        "\n",
        "    def classify(self, x):\n",
        "        logits = self.classifier(x)\n",
        "        return logits\n",
        "\n",
        "    def sample(self, z, y):\n",
        "        \"\"\"\n",
        "        Samples from the Decoder to generate an x.\n",
        "        :param z: latent normal variable\n",
        "        :param y: label (one-hot encoded)\n",
        "        :return: x\n",
        "        \"\"\"\n",
        "        y = y.float()\n",
        "        x = self.decoder(torch.cat([z, y], dim=1))\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "26PinGUROfAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_mnist(location, batch_size, labels_per_class):\n",
        "\n",
        "    n_labels = 10\n",
        "\n",
        "    flatten_bernoulli = lambda x: transforms.ToTensor()(x).view(-1).bernoulli()\n",
        "\n",
        "    mnist_train = MNIST(location, train=True, download=True,\n",
        "                        transform=flatten_bernoulli, target_transform=onehot(n_labels))\n",
        "    mnist_valid = MNIST(location, train=False, download=True,\n",
        "                        transform=flatten_bernoulli, target_transform=onehot(n_labels))\n",
        "\n",
        "    def get_sampler(labels, n=None):\n",
        "        # Only choose digits in n_labels\n",
        "        (indices,) = np.where(reduce(__or__, [labels == i for i in np.arange(n_labels)]))\n",
        "\n",
        "        # Ensure uniform distribution of labels\n",
        "        np.random.shuffle(indices)\n",
        "        # reorganize the data by labels and horizontally stack them so the data is like 0..0,1..1,2...2..so on\n",
        "        indices = np.hstack([list(filter(lambda idx: labels[idx] == i, indices))[:n] for i in range(n_labels)])\n",
        "        indices = torch.from_numpy(indices) # convert into a tensor\n",
        "        # Samples elements randomly from a given list of indices, without replacement.\n",
        "        sampler = SubsetRandomSampler(indices)\n",
        "        return sampler\n",
        "\n",
        "    # Dataloaders for MNIST\n",
        "    labelled = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, num_workers=2, \n",
        "                                           sampler=get_sampler(mnist_train.train_labels.numpy(), labels_per_class))\n",
        "    unlabelled = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, num_workers=2, \n",
        "                                             sampler=get_sampler(mnist_train.train_labels.numpy()))\n",
        "    validation = torch.utils.data.DataLoader(mnist_valid, batch_size=batch_size, num_workers=2, \n",
        "                                             sampler=get_sampler(mnist_valid.test_labels.numpy()))\n",
        "\n",
        "    return labelled, unlabelled, validation\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A7lNj4cRBKP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImportanceWeightedSampler(object):\n",
        "    \"\"\"\n",
        "    Importance weighted sampler [Burda 2015] to\n",
        "    be used in conjunction with SVI.\n",
        "    \"\"\"\n",
        "    def __init__(self, mc=1, iw=1):\n",
        "        \"\"\"\n",
        "        Initialise a new sampler.\n",
        "        :param mc: number of Monte Carlo samples\n",
        "        :param iw: number of Importance Weighted samples\n",
        "        \"\"\"\n",
        "        self.mc = mc\n",
        "        self.iw = iw\n",
        "\n",
        "    def resample(self, x):\n",
        "        return x.repeat(self.mc * self.iw, 1)\n",
        "\n",
        "    def __call__(self, elbo):\n",
        "        elbo = elbo.view(self.mc, self.iw, -1)\n",
        "        elbo = torch.mean(log_sum_exp(elbo, dim=1, sum_op=torch.mean), dim=0)\n",
        "        return elbo.view(-1)\n",
        "\n",
        "class SVI(nn.Module):\n",
        "    \"\"\"\n",
        "    Stochastic variational inference (SVI).\n",
        "    \"\"\"\n",
        "    base_sampler = ImportanceWeightedSampler(mc=1, iw=1)\n",
        "    def __init__(self, model, likelihood=F.binary_cross_entropy, beta=repeat(1), sampler=base_sampler):\n",
        "        \"\"\"\n",
        "        Initialises a new SVI optimizer for semi-supervised learning.\n",
        "        :param model: semi-supervised model to evaluate\n",
        "        :param likelihood: p(x|y,z) for example BCE or MSE\n",
        "        :param sampler: sampler for x and y, e.g. for Monte Carlo\n",
        "        :param beta: warm-up/scaling of KL-term\n",
        "        \"\"\"\n",
        "        super(SVI, self).__init__()\n",
        "        self.model = model\n",
        "        self.likelihood = likelihood\n",
        "        self.sampler = sampler\n",
        "        self.beta = beta\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        is_labelled = False if y is None else True\n",
        "\n",
        "        # Prepare for sampling\n",
        "        xs, ys = (x, y)\n",
        "\n",
        "        # Enumerate choices of label\n",
        "        if not is_labelled:\n",
        "            ys = enumerate_discrete(xs, self.model.y_dim)\n",
        "            xs = xs.repeat(self.model.y_dim, 1)\n",
        "\n",
        "        # Increase sampling dimension\n",
        "        xs = self.sampler.resample(xs)\n",
        "        ys = self.sampler.resample(ys)\n",
        "\n",
        "        reconstruction = self.model(xs, ys)\n",
        "\n",
        "        # p(x|y,z)\n",
        "        likelihood = -self.likelihood(reconstruction, xs)\n",
        "\n",
        "        # p(y)\n",
        "        prior = -log_standard_categorical(ys)\n",
        "\n",
        "        # Equivalent to -L(x, y)\n",
        "        elbo = likelihood + prior - next(self.beta) * self.model.kl_divergence\n",
        "        L = self.sampler(elbo)\n",
        "\n",
        "        if is_labelled:\n",
        "            return torch.mean(L)\n",
        "\n",
        "        logits = self.model.classify(x)\n",
        "\n",
        "        L = L.view_as(logits.t()).t()\n",
        "\n",
        "        # Calculate entropy H(q(y|x)) and sum over all labels\n",
        "        H = -torch.sum(torch.mul(logits, torch.log(logits + 1e-8)), dim=-1)\n",
        "        L = torch.sum(torch.mul(logits, L), dim=-1)\n",
        "\n",
        "        # Equivalent to -U(x)\n",
        "        U = L + H\n",
        "        return torch.mean(U)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFkSgkUaV1kN",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate M1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oly5HeNVuBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_mnist_VAE(location, batch_size, labels_per_class):\n",
        "\n",
        "    n_labels = 10\n",
        "\n",
        "    flatten_bernoulli = lambda x: transforms.ToTensor()(x).view(-1).bernoulli()\n",
        "\n",
        "    mnist_train = MNIST(location, train=True, download=True,\n",
        "                        transform=flatten_bernoulli, target_transform=onehot(n_labels))\n",
        "    mnist_valid = MNIST(location, train=False, download=True,\n",
        "                        transform=flatten_bernoulli, target_transform=onehot(n_labels))\n",
        "\n",
        "    def get_indices(labels, n=None):\n",
        "        # Only choose digits in n_labels\n",
        "        (indices,) = np.where(reduce(__or__, [labels == i for i in np.arange(n_labels)]))\n",
        "\n",
        "        # Ensure uniform distribution of labels\n",
        "        np.random.shuffle(indices)\n",
        "        # reorganize the data by labels and horizontally stack them so the data is like 0..0,1..1,2...2..so on\n",
        "        indices = np.hstack([list(filter(lambda idx: labels[idx] == i, indices))[:n] for i in range(n_labels)])\n",
        "        indices = torch.from_numpy(indices) # convert into a tensor\n",
        "        return indices\n",
        "    \n",
        "    # Datasets for MNIST, convert those into numpy arrayes for the SVM \n",
        "    indices =  get_indices(mnist_train.train_labels.numpy(), labels_per_class)\n",
        "    x_train = mnist_train.data.numpy()\n",
        "    y_train = mnist_train.targets.numpy()\n",
        "    labelled_train_x = x_train[indices]\n",
        "    #labelled_x = [i.flatten() for i in labelled_x]\n",
        "    labelled_train_y = y_train[indices]\n",
        "    # l,w,h = x_train.shape\n",
        "    # mask = np.zeros(l,dtype=bool) \n",
        "    # mask[indices] = True\n",
        "    # unlabelled_train_x = x_train[~mask]\n",
        "    # #unlabelled_x = [i.flatten() for i in unlabelled_x]\n",
        "    # unlabelled_train_y = y_train[~mask]\n",
        "    train = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, num_workers=2) \n",
        "    x_valid = mnist_valid.data.numpy()\n",
        "    y_valid = mnist_valid.targets.numpy()\n",
        "\n",
        "    return train, labelled_train_x, labelled_train_y, x_valid, y_valid\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-78sFnaVuJx",
        "colab_type": "code",
        "outputId": "5de82d97-9ffd-43bd-bcc4-7064beb8c493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train M1 model Variational encoder\n",
        "# number of labelled instances per class, there are 10000 instances in total in train so N label is smaller\n",
        "N_labels = [10, 50, 100, 200, 300, 400, 500]\n",
        "M1_accuracys = []\n",
        "total_losses = {}\n",
        "\n",
        "for n_label in N_labels: \n",
        "    print(\"labels per class: \", n_label)\n",
        "    total_losses[n_label] = []\n",
        "    model = VariationalAutoencoder([784, 32, [256, 128]]) # generative\n",
        "    clf = LogisticRegression(multi_class='multinomial') # discrimative\n",
        "    train, labelled_train_x, labelled_train_y, x_valid, y_valid = get_mnist_VAE(location=\"./\", batch_size=32, labels_per_class=n_label)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.999))\n",
        "\n",
        "    for epoch in range(5):\n",
        "        # tell the model to train\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        # VAE training doesn't require y and we go through all training examples\n",
        "\n",
        "        for (x, y) in iter(train): \n",
        "            \n",
        "            # Generative Training \n",
        "            x = Variable(x)\n",
        "\n",
        "            reconstruction = model(x)\n",
        "            \n",
        "            likelihood = -binary_cross_entropy(reconstruction, x)\n",
        "            elbo = likelihood - model.kl_divergence\n",
        "            \n",
        "            L = -torch.mean(elbo)\n",
        "\n",
        "            L.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            total_loss += torch.Tensor.item(L.data)        \n",
        "        m = len(train)\n",
        "        # at the end of the epoch\n",
        "        print(f\"Epoch: {epoch}\\tL: {total_loss/m:.2f}\")   \n",
        "        \n",
        "        total_losses[n_label].append(total_loss)\n",
        "\n",
        "    # Discriminative Training (train once with all labelled data and z generated by fully trained generator in this epoch)\n",
        "    n, l, w =  labelled_train_x.shape\n",
        "    labelled_train_x.shape = (n, l*w)\n",
        "    labelled_train_x_tensor = torch.from_numpy(labelled_train_x)\n",
        "    # use the trained model to obtain the latent representation of x\n",
        "    x_latent = model(Variable(labelled_train_x_tensor.float())) \n",
        "    class_labels = labelled_train_y\n",
        "    clf.fit(x_latent.detach().numpy(), class_labels)\n",
        "    \n",
        "    ##### validate when all epoches are done\n",
        "    # reshape the labelled x \n",
        "    n, l, w = x_valid.shape\n",
        "    x_valid.shape = (n, l*w)\n",
        "    x_valid_tensor = torch.from_numpy(x_valid)\n",
        "\n",
        "    # use the trained model to obtain the latent representation of x\n",
        "    x_latent = model(Variable(x_valid_tensor.float()))\n",
        "\n",
        "    # use trained clf to predict y \n",
        "    y_pred = clf.predict(x_latent.detach().numpy())\n",
        "\n",
        "    # calculate accuracy\n",
        "    accuracy = sum(y_pred == y_valid)/len(y_valid)\n",
        "    print(\"accuray: \", accuracy)\n",
        "    M1_accuracys.append(accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "labels per class:  10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tL: 181.48\n",
            "Epoch: 1\tL: 151.33\n",
            "Epoch: 2\tL: 144.59\n",
            "Epoch: 3\tL: 141.17\n",
            "Epoch: 4\tL: 138.79\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuray:  0.8216\n",
            "labels per class:  50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tL: 181.71\n",
            "Epoch: 1\tL: 151.73\n",
            "Epoch: 2\tL: 145.08\n",
            "Epoch: 3\tL: 141.50\n",
            "Epoch: 4\tL: 139.09\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuray:  0.8544\n",
            "labels per class:  100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tL: 181.38\n",
            "Epoch: 1\tL: 150.81\n",
            "Epoch: 2\tL: 144.65\n",
            "Epoch: 3\tL: 141.09\n",
            "Epoch: 4\tL: 138.67\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuray:  0.893\n",
            "labels per class:  200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tL: 181.08\n",
            "Epoch: 1\tL: 150.84\n",
            "Epoch: 2\tL: 144.46\n",
            "Epoch: 3\tL: 141.00\n",
            "Epoch: 4\tL: 138.76\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuray:  0.9013\n",
            "labels per class:  300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tL: 181.52\n",
            "Epoch: 1\tL: 151.41\n",
            "Epoch: 2\tL: 144.79\n",
            "Epoch: 3\tL: 141.37\n",
            "Epoch: 4\tL: 139.06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuray:  0.9027\n",
            "labels per class:  400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tL: 182.26\n",
            "Epoch: 1\tL: 151.86\n",
            "Epoch: 2\tL: 144.66\n",
            "Epoch: 3\tL: 140.91\n",
            "Epoch: 4\tL: 138.52\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuray:  0.9046\n",
            "labels per class:  500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tL: 181.60\n",
            "Epoch: 1\tL: 152.10\n",
            "Epoch: 2\tL: 145.40\n",
            "Epoch: 3\tL: 141.81\n",
            "Epoch: 4\tL: 139.40\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuray:  0.9054\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eFBCWo7uKQX",
        "colab_type": "code",
        "outputId": "9af0237d-14f0-4449-b1a6-2d67471a3b42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "N_labels = [10, 50, 100, 200, 300, 400, 500]\n",
        "N_labels_scaled = [i*10 for i in N_labels] # 10 for each class so total number of labelled samples should times 10\n",
        "M1_err = [1-i for i in M1_accuracys]\n",
        "plt.plot(N_labels_scaled, M1_err)\n",
        "plt.xlabel(\"labelled samples\")\n",
        "plt.ylabel(\"testing error\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'testing error')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de3hc9X3n8fd3LrpYku+yRvIlxtwtISdE3EJwCAFbBAGbXZJA0m1od0u3bZ52nz50Q56k9Gk2T7tp2m5Cm6eFpnmaLqWE0jQBG7ANgQAJN5ngi3zHNvgmWb7bknX/7h/nyIzlsT2yNTrSzOf1PPPMmd85Z+Z7ZHk+Or9zzu+YuyMiIjJULOoCRERkbFJAiIhIRgoIERHJSAEhIiIZKSBERCSjRNQFjJTp06f73Llzoy5DRGRcWbly5T53r8w0L28CYu7cuTQ3N0ddhojIuGJm751unrqYREQko5wGhJk1mtlGM9tiZg9kmL/QzN42sz4zu2vIvL8wsxYzW29mD5mZ5bJWERE5Wc4CwsziwPeAW4H5wD1mNn/IYu8D9wKPDVn3Y8D1QD1QB1wFfCJXtYqIyKlyeQziamCLu28FMLPHgTuBdYMLuPv2cN7AkHUdKAGKAAOSQFsOaxURkSFy2cU0E9iR9npn2HZW7v4a8CKwJ3wsc/f1Q5czs/vMrNnMmtvb20egZBERGTQmD1Kb2UXA5cAsglC5ycxuGLqcuz/i7g3u3lBZmfEsLREROUe5DIhdwOy017PCtmx8Bnjd3Y+5+zHgWeC6Ea5PRETOIJcB8RZwsZldYGZFwN3AU1mu+z7wCTNLmFmS4AD1KV1MI+FQZw/ffX4z63YfycXbi4iMWzkLCHfvA74MLCP4cn/C3VvM7BtmdgeAmV1lZjuBzwIPm1lLuPqTwLvAGmAVsMrdn85FnWbG3764mZ+uynbnRkSkMFi+3DCooaHBz/VK6i/94E3e29/Bi/ffiC63EJFCYmYr3b0h07wxeZB6tDXWpdi+v5MNrUejLkVEZMxQQAC3zK/CDJ5b2xp1KSIiY4YCApheXsxVc6eyrEUBISIySAERaqxNsaH1KNv2dURdiojImKCACDXWpQC0FyEiElJAhGoml7Jg1iSe1XEIERFAAXGSxXUpVu04xO5Dx6MuRUQkcgqINI21QTfTcnUziYgoINLNqyznkqpynlNAiIgoIIZqrKvmzW0H2H+sO+pSREQipYAYorE2xYDDinW6P5GIFDYFxBCXV1cwZ+oEdTOJSMFTQAxhZjTWpfjFln0c6eqNuhwRkcgoIDJYXJuit995ccPeqEsREYmMAiKDj8yeTNXEYp5do24mESlcCogMYjFjcW2Klzbt5XhPf9TliIhEQgFxGo21Kbp6B/j5pvaoSxERiYQC4jSuvmAqkyckNXifiBQsBcRpJOIxbrm8iufXt9HTNxB1OSIio04BcQa3XpHiaFcfv3x3X9SliIiMOgXEGXzswumUFyfUzSQiBUkBcQYlyTifvGwGy1va6B/wqMsRERlVCoizaKxNsb+jh+btB6IuRURkVCkgzuLGSyspTsR0pzkRKTgKiLMoK06w8JJKlrW04q5uJhEpHAqILDTWpthzuIvVOw9HXYqIyKhRQGThU5fPIBEzDQEuIgVFAZGFyROKuO7CaTy3Vt1MIlI4FBBZWlybYtu+Dja1HYu6FBGRUaGAyNKi2irM4DmdzSQiBUIBkaUZFSU0fGiKjkOISMFQQAzD4toU6/cc4b39HVGXIiKScwqIYVhcmwLQ2EwiUhAUEMMwe+oE6mZO1FXVIlIQFBDD1Fib4lfvH6L1cFfUpYiI5JQCYpga66oBWL5OexEikt8UEMN00YxyLppRrtNdRSTvKSDOQWNtije2HeBAR0/UpYiI5ExOA8LMGs1so5ltMbMHMsxfaGZvm1mfmd01ZN4cM1tuZuvNbJ2Zzc1lrcPRWJeif8B5fl1b1KWIiORMzgLCzOLA94BbgfnAPWY2f8hi7wP3Ao9leIt/Br7t7pcDVwN7c1XrcNXWTGTWlFJdNCcieS2XexBXA1vcfau79wCPA3emL+Du2919NTCQ3h4GScLdV4TLHXP3zhzWOixmRmNtilc37+NoV2/U5YiI5EQuA2ImsCPt9c6wLRuXAIfM7Mdm9isz+3a4RzJmNNal6Okf4MWN7VGXIiKSE2P1IHUCuAG4H7gKmEfQFXUSM7vPzJrNrLm9fXS/qK+cM4XKimKeW7tnVD9XRGS05DIgdgGz017PCtuysRN4J+ye6gN+Alw5dCF3f8TdG9y9obKy8rwLHo5YzFg0v4oXN7TT1ds/qp8tIjIachkQbwEXm9kFZlYE3A08NYx1J5vZ4Lf+TcC6HNR4XhrrUhzv7eflTepmEpH8k7OACP/y/zKwDFgPPOHuLWb2DTO7A8DMrjKzncBngYfNrCVct5+ge+kFM1sDGPAPuar1XF07bxqTSpM6m0lE8lIil2/u7s8AzwxpezBt+i2CrqdM664A6nNZ3/lKxmPcfHkVK9a10ts/QDI+Vg/piIgMn77RzlNjXYojXX289u7+qEsRERlRCojzdMPF05lQFFc3k4jkHQXEeSpJxvnkpTNY3tJG/4BHXY6IyIhRQIyAxroU+4518/b7B6MuRURkxCggRsAnL5tBUTymIcBFJK8oIEZAeXGCGy6eznNrW3FXN5OI5AcFxAhZXJdi16HjrN11JOpSRERGhAJihNxyeRXxmPFci8ZmEpH8oIAYIVPKirh23lQdhxCRvKGAGEGNtSnebe9gc9vRqEsRETlvCogRtKg2BaC9CBHJCwqIEVQ1sYQr50zWVdUikhcUECOssS5Fy+4j7DgwZu6QKiJyThQQI6yxthqAZdqLEJFxTgExwuZMm8D86ok8q+MQIjLOKSByoLEuxcr3DrL3SFfUpYiInDMFRA401gVnMy1b1xZxJSIi504BkQMXzyhn3vQylqmbSUTGMQVEDpgZjXUpXtu6n0OdPVGXIyJyThQQOdJYl6J/wFmhbiYRGacUEDlyxcxJ1Ewq0emuIjJunTEgzCxuZhtGq5h8YmYsrkvx8uZ9HOvui7ocEZFhO2NAuHs/sNHM5oxSPXmlsTZFT98AL23cG3UpIiLDlshimSlAi5m9CXQMNrr7HTmrKk80zJ3K9PIinlvbSlN9TdTliIgMSzYB8cc5ryJPxWPGLfNTPPXOLrp6+ylJxqMuSUQka2c9SO3uPwc2ABXhY33YJllorEvR0dPPq5v3RV2KiMiwnDUgzOxzwJvAZ4HPAW+Y2V25LixfXDdvGhUlCQ0BLiLjTjZdTF8DrnL3vQBmVgk8DzyZy8LyRVEixs2XV/H8+jZ6+wdIxnVmsYiMD9l8W8UGwyG0P8v1JNRYl+JQZy+vb90fdSkiIlnL5ov+OTNbZmb3mtm9wFLgmdyWlV8+cUklZUVxlqzaE3UpIiJZO9uFcgY8BDwM1IePR9z9K6NQW94oSca5ZX4Vz7W00tM3EHU5IiJZOeMxCHd3M3vG3a8AfjxKNeWlpvoafvLObn6xZR+fvGxG1OWIiJxVNl1Mb5vZVTmvJM/dcMl0JpYkeHrV7qhLERHJSjZnMV0DfNHM3iO4ktoIdi7qc1pZnilOxFlcm+LZta26aE5ExoUzBkR4DOI+4L3RKSe/NS2o4d9W7uTnm9pZXJuKuhwRkTM622B9DnzP3d8b+hil+vLKxy6cxtSyInUzici4oGMQoygZj9FYl+KF9Xvp7NEQ4CIytmUTENcAr5vZu2a22szWmNnqXBeWr5rqqzne28/PNmgIcBEZ27I5SL0451UUkGsumEZlRTFPr9qtIcBFZEzLZjTX94DZwE3hdGc26wGYWaOZbTSzLWb2QIb5C83sbTPryzQAoJlNNLOdZva32XzeeBCPGbddUc2LG9s52tUbdTkiIqeVzWiufwJ8Bfhq2JQEHs1ivTjwPeBWYD5wj5nNH7LY+8C9wGOneZv/Dbx8ts8ab5rqq+npG+D59W1RlyIiclrZ7Al8BriD8G5y7r6b4L4QZ3M1sMXdt7p7D/A4cGf6Au6+3d1XA6eMP2FmHwWqgOVZfNa4cuWcKdRMKuFpjc0kImNYNgHRE57u6gBmVpble88EdqS93hm2nZWZxYC/Au4/y3L3mVmzmTW3t7dnWVb0YjHjtvpqXtnczuFOdTOJyNiUTUA8YWYPA5PN7LcI7gXxD7kti98FnnH3nWdayN0fcfcGd2+orKzMcUkjq6m+ht5+Z5luJCQiY9RZz2Jy9780s1uAI8ClwIPuviKL995FcHB70KywLRvXATeY2e8C5UCRmR1z91MOdI9X9bMmMWfqBJ5evZvPXTX77CuIiIyybE5zJQyEbEIh3VvAxWZ2AUEw3A18IcvP++LgdHgPioZ8CgcAM6OpvpqHX97K/mPdTCsvjrokEZGT5OzOcO7eB3wZWAasB55w9xYz+4aZ3QFgZleZ2U6C+10/bGYtuapnLGqqr6F/wHl2rbqZRGTsseD48/jX0NDgzc3NUZcxLO7OzX/9c6aXF/Oj374u6nJEpACZ2Up3b8g0T/eWjlDQzVTDm9sP0HakK+pyREROks2FcmvCMZjSH6+Y2f81s2mjUWQ+u31BNe7wzBpdEyEiY0s2exDPAkuBL4aPp4FmoBX4p5xVViAumlHBZakKlqxWQIjI2JLNWUw3u/uVaa/XmNnb7n6lmf1argorJLcvqOHbyzay69BxZk4ujbocEREguz2IuJldPfgivDfE4P0ydVODEdBUXw3A0tW6kZCIjB3ZBMR/B/7RzLaZ2XbgH4HfCofc+PNcFlcoPjStjPpZk9TNJCJjSjbDfb/l7lcAHwYWuHu9u7/p7h3u/kTuSywMTfXVrN55mO37OqIuRUQEyO4spmIz+wLwe8AfmNmDZvZg7ksrLLeFNw9aqrOZRGSMyKaL6acEw3T3EQz5PfiQETRzcikf/dAUnl6l4xAiMjZkcxbTLHdvzHklQlN9NX/69Dq27D3KRTOyueWGiEjuZLMH8UszuyLnlQifvqIaM3QjIREZE7IJiI8DK8N7S68evLI614UVoqqJJVxzwVSWrN5NvoyRJSLjVzZdTLfmvAo5oam+hq//ZC3r9xxlfs3EqMsRkQJ22j0IMxv8djp6mofkwK11KeIxY4kumhORiJ2pi+mx8HklwdhLK9Me42tc7XFkWnkxH7twGktW71E3k4hE6rQB4e5N4fMF7j4vfB58zBu9EgvP7fU1vH+gk9U7D0ddiogUsGwulHshmzYZOYtrUyTj6mYSkWid6RhEiZlNBaab2RQzmxo+5gIzR6vAQjRpQpKFF1eydPUeBgbUzSQi0TjTHsRvExxvuIyTjz/8FPjb3JdW2JoWVLP7cBdvv38w6lJEpECd6RjEd939AuD+IccgFri7AiLHbr68iqJETCO8ikhksrlQrtXMKgDM7Otm9mMzu/JsK8n5qShJctOlM1i6Zg/96mYSkQhkExB/7O5HzezjwM0E94P4u9yWJRB0M7Uf7eaNbfujLkVEClA2AdEfPt8GPOLuS4Gi3JUkg266bAalybi6mUQkEtkExC4zexj4PPCMmRVnuZ6cpwlFCW6eX8Vza1vp7R+IuhwRKTDZfNF/DlgGLHb3Q8BU4I9yWpWc0FRfzYGOHn75rrqZRGR0ZXPL0U5gL8GorhDcOGhzLouSD3zikkoqihMs0Y2ERGSUZXMl9Z8AXwG+GjYlgUdzWZR8oCQZ55baKpa1tNLd13/2FURERkg2XUyfAe4gvM2ou+8GdLuzUXR7fQ1Huvp4ZdO+qEsRkQKSTUD0eDCsqAOYWVluS5Khrr9oOpNKkxqbSURGVTYB8UR4FtNkM/st4Hng+7ktS9IVJWLcWpdixbo2unrVzSQioyObg9R/CTwJ/DtwKfCguz+U68LkZE31NXT09PPihr1RlyIiBSKbg9TfcvcV7v5H7n6/u68ws2+NRnHygWvnTWVaWZEumhORUZNNF9MtGdp0n+pRlojH+PQV1bywoY2O7r6oyxGRAnCm+0H8jpmtAS41s9Vpj23A6tErUQY11VfT1TvAC+pmEpFRkDjDvMeAZ4E/Bx5Iaz/q7gdyWpVkdNXcqVRNLObpVbu5Y0FN1OWISJ47bUC4+2HgMHDP6JUjZxKLGbddUcOjr7/Hka5eJpYkoy5JRPKYBt0bZ5oWVNPTP8CKlraoSxGRPKeAGGc+MnsyMyeX8rQumhORHMtpQJhZo5ltNLMtZvZAhvkLzextM+szs7vS2j9sZq+ZWUt4YPzzuaxzPDEzmhZU8+rmfRzs6Im6HBHJYzkLCDOLA98jOCV2PnCPmc0fstj7wL0EB8TTdQK/7u61QCPwHTObnKtax5vb62voG3CWtbRGXYqI5LFc7kFcDWxx963u3gM8DtyZvoC7b3f31cDAkPZN7r45nN5NMNx4ZQ5rHVdqayYyd9oEdTOJSE7lMiBmAjvSXu8M24bFzK4muMXpuxnm3WdmzWbW3N7efs6FjjdmRlN9Da+9u5/2o91RlyMieWpMH6Q2s2rg/wG/4e6n3HPT3R9x9wZ3b6isLKwdjNsX1DDg8NxaDb0hIrmRy4DYBcxOez0rbMuKmU0ElgJfc/fXR7i2ce/SVAUXzyjn6VUKCBHJjVwGxFvAxWZ2gZkVAXcDT2WzYrj8fwD/7O5P5rDGca2pvoa33jtA6+GuqEsRkTyUs4Bw9z7gy8AyYD3whLu3mNk3zOwOADO7ysx2Ap8FHjazlnD1zwELgXvN7J3w8eFc1TpeNS2oxh2WrtFehIiMPAtuFjf+NTQ0eHNzc9RljLpPf/cVihIxfvJ710ddioiMQ2a20t0bMs0b0wep5eyaFlTzzo5D7DjQGXUpIpJnFBDj3O31waiu6mYSkZGmgBjnZk+dwILZk3l6lS6aE5GRpYDIA7fXV9Oy+wjb9nVEXYqI5BEFRB64rb4agCXaixCREaSAyAPVk0q5au4Ujc0kIiNKAZEnmupr2NR2jE1tR6MuRUTyhAIiT9x6RYqYqZtJREaOAiJPzKgo4dp503h69R7y5eJHEYmWAiKPNNXXsG1fBy27j0RdiojkAQVEHmmsS5GIGUtW66I5ETl/Cog8MrWsiOsvms6S1bvVzSQi500BkWea6qvZefA47+w4FHUpIjLOKSDyzKLaFEXxmLqZROS8KSDyzKTSJAsvqWTp6j0MDKibSUTOnQIiD92+oJrWI100v3cw6lJEZBxTQOShmy+voiQZY4mG3hCR86CAyENlxQluumwGz6zZQ1//QNTliMg4pYDIU031New71sMb2w5EXYqIjFMKiDz1yUtnUFYUVzeTiJwzBUSeKi2Kc/P8Kp5d20qvuplE5BwoIPJYU30Nhzp7eXXLvqhLEZFxSAGRxxZeMp2KkgRLVumiOREZPgVEHitOxFlcm2J5Syvdff1RlyMi44wCIs/dvqCGo9193PbQq/zlso2s2XlYA/mJSFYsX74sGhoavLm5Oeoyxhx350dv7eAn7+zizW0HGHComVTCotoUi2qruHruVBJx/Z0gUqjMbKW7N2Scp4AoHAc6enhhfRvLWtp4ZXM73X0DTJmQ5FOXV7FofhULL6mkJBmPukwRGUUKCDlFZ08fL29qZ1lLGy+sb+NIVx+lyTifuKSSRbVVfOqyKiZNSEZdpojk2JkCIjHaxcjYMKEoQWNdNY111fT2D/D61v0sb2lj+bpWnmtpJREzrp03jUW1VSyanyI1qSTqkkVklGkPQk4yMOCs2nmI5evaWNbSytb2DgAWzJ7MovlVLK5NcdGM8oirFJGRoi4mOWdb9h5lWUsby1taWbXzMAAXVpaxqDbF4toU9TMnEYtZxFWKyLlSQMiI2HP4OCvCPYvXtx6gf8BJTSw50Q11zbypJHVGlMi4ooCQEXeos4efbdjLspZWfr6pna7eASaWJPjU5VUsrg3OiJpQpENcImOdAkJy6nhPPy9vbmd5SxsvbGjjUGcvxYkYCy+pZNH8Km6+vIopZUVRlykiGegsJsmp0qJgSI/FtSn6+gd4c9uBEwe5V6xrIx4zrp47NeiKqk0xc3Jp1CWLSBa0ByE54+6s2XWY5S1BWGzeewyAK2ZOCs6Iqktx8YxyzHSQWyQq6mKSMWFr+7ETexa/ev8QABdML2PR/GDP4iOzJ+uMKJFRpoCQMaftSBfL1wWnz7727n76BpzKimJuCa+1uG7eNIoSOiNKJNciCwgzawS+C8SB77v7/xkyfyHwHaAeuNvdn0yb9yXg6+HLb7r7D8/0WQqI8evw8V5e3LCX5etaeWljO509/VSUJLjpshksmp/ixksrKSvW4TKRXIgkIMwsDmwCbgF2Am8B97j7urRl5gITgfuBpwYDwsymAs1AA+DASuCj7n7wdJ+ngMgPXb39vLp5H8vXtfL8+r0c6OihKBHjhoums6g2OCNqWnlx1GWK5I2ozmK6Gtji7lvDIh4H7gROBIS7bw/nDb1p8mJghbsfCOevABqBf81hvTIGlCSDe2nfPL+Kvv4Bmt87yLKW1vAU2r3EbA0Nc6eeGPZj9tQJUZcskrdyGRAzgR1pr3cC15zHujOHLmRm9wH3AcyZM+fcqpQxKxGPce28aVw7bxoPNs2nZfcRlre0snxdG99cup5vLl3P/OqJLKoNwuKyVIXOiBIZQeO6Y9fdHwEegaCLKeJyJIfMjLqZk6ibOYk/XHQp2/d1sHxdsGfx3Rc2853nNzNn6oQTp89eOWcKcZ0RJXJechkQu4DZaa9nhW3ZrnvjkHVfGpGqJC/MnV7GfQsv5L6FF7L3aBfPrwsOcv/wte18/9VtTC8v4lOXVTF3ehnlxXHKSxKUFSUoL0lQUZykLGyrKE5Skoxpz0Mkg1wGxFvAxWZ2AcEX/t3AF7JcdxnwZ2Y2JXy9CPjqyJco+WBGRQlfuGYOX7hmDke7enlxYzvLW1pZumYPx7r7zrp+zKC8OBE8SoLnsuIEFenT4fPg/KHLDy5XnFDYSP7IWUC4e5+ZfZngyz4O/MDdW8zsG0Czuz9lZlcB/wFMAW43sz9191p3P2Bm/5sgZAC+MXjAWuRMKkqS3LGghjsW1ODudPcNcLSrj2PdfXR093G0K3g+1t3H0bDtWDj/WDjd0RMst+dw1wfze/rI5oS/ZNxOhEV5GDJlxacPlfKSIQGUto5GxpWo6UI5kSwMDDjHe/tPCpJMoXIsLXROBNCQdTp7+rP6zOJE7KSAGQySTKEyNHQmliSZMiHJpNIkCQWNnIEG6xM5T7GYURZ+IVed53v1DzgdPWGwDNmTybhXk7b303qki472cA+oq4/uvqFniJ+qoiTB5AlJpkwoYlJp8Dx5QpLJE4qYXJpkSlmSyaVFTAqXmVyaZGJpUgf5RQEhMtriMWNiSZKJJcnzfq/e/oFT9lKOdvdx5HgvhzqDx8HOHg4fD54Pdfay40Anh473cvh472m7zcz4YC9kQhFTJiSZXBqGyoRTQ2ZwuqI4ofG08ogCQmQcS8Zj4Zf28O+30T/gHO3q5WBnL4fC8Dh0vCcMlV4Od/YE8473cqCjh63tHRzs7OFo1+kP/MdjxqTS5EmhMXlCsIcyZcKpbZPDtvLihA7uj0EKCJECFY9ZWriUZb1eX/8Ah48HwXEiWDLsqRzq7GXv0S42th7l8PHeM55RlojZkD2SwT2VU0NmUtoeTGkyrmDJIQWEiAxLIh5jWnnxsMfE6ukLg6Wzh0PHeznYETwfDsMlPXB2HTrOut2HOdjZy/He0x/UL0rETtpbKS9OUBSPUZyMUZyIUZyIU5yIUTQ4HbYXpc0rTsQoTsYzrlectl5RPFZw3WcKCBEZFUWJGJUVxVRWDC9Yunr7w2D5YO/k8PGw+yute+xgZw/7jnXT3TtAd18/3X0DdPcN0NMXvO7tP/8zNpNxO2uwFJ0IlpPDJVgmfmpAJTOvd+J1+nqjHFIKCBEZ00qScUqScaomlpzX+/QPOD1pgdEdPnf1DtDTP3BSsPSE4dLd1x+2n7reiWXS1hsMs1Pepzd43Tdw/iFVFP8gOILgiVM3cxJ/c89Hzvu9h1JAiEhBiMeM0qI4pUVx4PzPIDsXgyF1Imh6B+jpD0Kquy9DQPVmCKy09QbDa/bU3NznXQEhIjJKTg6psU+XWIqISEYKCBERyUgBISIiGSkgREQkIwWEiIhkpIAQEZGMFBAiIpKRAkJERDLKmzvKmVk78N5ZFpsO7BuFcsaiQt12bXdh0XYP34fcvTLTjLwJiGyYWfPpbq2X7wp127XdhUXbPbLUxSQiIhkpIEREJKNCC4hHoi4gQoW67druwqLtHkEFdQxCRESyV2h7ECIikiUFhIiIZFQwAWFmjWa20cy2mNkDUddzvszsB2a218zWprVNNbMVZrY5fJ4StpuZPRRu+2ozuzJtnS+Fy282sy9FsS3DYWazzexFM1tnZi1m9gdhe15vu5mVmNmbZrYq3O4/DdsvMLM3wu37kZkVhe3F4est4fy5ae/11bB9o5ktjmaLhsfM4mb2KzNbEr7O++02s+1mtsbM3jGz5rBtdH/P3T3vH0AceBeYBxQBq4D5Udd1ntu0ELgSWJvW9hfAA+H0A8C3wulPA88CBlwLvBG2TwW2hs9TwukpUW/bWba7GrgynK4ANgHz833bw/rLw+kk8Ea4PU8Ad4ftfw/8Tjj9u8Dfh9N3Az8Kp+eHv//FwAXh/4t41NuXxfb/IfAYsCR8nffbDWwHpg9pG9Xf80LZg7ga2OLuW929B3gcuDPims6Lu78MHBjSfCfww3D6h8B/Smv/Zw+8Dkw2s2pgMbDC3Q+4+0FgBdCY++rPnbvvcfe3w+mjwHpgJnm+7WH9x8KXyfDhwE3Ak2H70O0e/Hk8CXzKzCxsf9zdu919G7CF4P/HmGVms4DbgO+Hr40C2O7TGNXf80IJiJnAjrTXO8O2fFPl7nvC6VagKpw+3faP659L2H3wEYK/pvN+28NulneAvQT/0d8FDrl7X7hI+jac2L5w/mFgGuNwu4HvAP8LGAhfT6MwttuB5Wa20szuC9tG9fc8cS5Vy9jn7m5meXsOs5mVA/8O/E93PxL8kRjI1213937gw2Y2GfgP4LKIS8o5M2sC9rr7Ssh8NUAAAAVpSURBVDO7Mep6RtnH3X2Xmc0AVpjZhvSZo/F7Xih7ELuA2WmvZ4Vt+aYt3K0kfN4btp9u+8flz8XMkgTh8C/u/uOwuSC2HcDdDwEvAtcRdCUM/qGXvg0nti+cPwnYz/jb7uuBO8xsO0HX8E3Ad8n/7cbdd4XPewn+ILiaUf49L5SAeAu4ODzzoYjg4NVTEdeUC08Bg2cpfAn4aVr7r4dnOlwLHA53U5cBi8xsSng2xKKwbcwK+5P/EVjv7n+dNiuvt93MKsM9B8ysFLiF4PjLi8Bd4WJDt3vw53EX8DMPjlo+Bdwdnu1zAXAx8ObobMXwuftX3X2Wu88l+H/7M3f/Inm+3WZWZmYVg9MEv59rGe3f86iP1I/Wg+Ao/yaCftuvRV3PCGzPvwJ7gF6CfsX/RtDX+gKwGXgemBoua8D3wm1fAzSkvc9vEhyw2wL8RtTblcV2f5ygb3Y18E74+HS+bztQD/wq3O61wINh+zyCL7otwL8BxWF7Sfh6Szh/Xtp7fS38eWwEbo1624bxM7iRD85iyuvtDrdvVfhoGfzOGu3fcw21ISIiGRVKF5OIiAyTAkJERDJSQIiISEYKCBERyUgBISIiGSkgZNwzs2NnmT/X0ka9zfI9/8nM7gqnXzKzrG8Ib2Y3Do46GqWz/VxEzkYBISIiGSkgJG+YWbmZvWBmb4fj6KeP2Jsws38xs/Vm9qSZTQjX+aiZ/TwcEG3Z4DAGZ/iMRWb2WvgZ/xaOCTV4v5ENZvY28J9Ps26tBfd0eCccs//isP0n4ee3pA3KhpkdM7Nvh+3Pm9nV4d7MVjO7I1zmXjP7adi+2cz+5DSf/Udm9lb4uYP3kigzs6UW3GNirZl9PvufthSEqK8Y1EOP830Ax8LnBDAxnJ5OcOWoAXMJrr6+Ppz3A+B+giGzfwlUhu2fB34QTv8TcFc4/RLQEL7ny0BZ2P4V4EGCq3d3EAzfYAT3KliSoc6/Ab4YThcBpeH04NWwpQRXSU8LXzvhFb8EY/EsD2teALwTtt9LcEX9tLT1G4b8XBYR3NTeCP4oXEJwP5H/AvxDWn2Tov631GNsPTSaq+QTA/7MzBYSDA09kw+GQ97h7r8Ipx8Ffh94DqgjGCkTghtL7eH0riW48cwvwuWLgNcIRlXd5u6bAczsUeC+DOu/BnzNgvsb/HhweeD3zewz4fRsgqDZD/SENUIwfEK3u/ea2RqC0Bu0wt33h5/9Y4LhSJrT5i8KH78KX5eHn/EK8Fdm9i2CQHvlDNsuBUgBIfnki0Al8NHwi3Q7wV/3EPw1ns4JAqXF3a/L8v2N4Mv4npMazT6czcru/piZvUFw85tnzOy3CYLsZuA6d+80s5fSau5198G6B4Du8H0G0kYyPd22Da37z9394VM2KLg15aeBb5rZC+7+jWy2RQqDjkFIPplEcO+AXjP7JPChtHlzzGwwCL4AvEowaFvlYLuZJc2s9gzv/zpwvZldFC5fZmaXABuAuWZ2YbjcPZlWNrN5wFZ3f4hgFM76sOaDYThcRrCXMly3WHCv4lKCO4z9Ysj8ZcBvph0vmWlmM8ysBuh090eBbxPcwlbkBO1BSD75F+DpsAummeCLe9BG4PfM7AfAOuDv3L0nPJX1ITObRPD/4TsEo2eewt3bzexe4F/NrDhs/rq7bwoPLi81s06CrpuKDG/xOeC/mlkvwd3A/gzoAP6Hma0Pa3z9HLb7TYL7Y8wCHnX39O4l3H25mV0OvBZ2jR0Dfg24CPi2mQ0QjAr8O+fw2ZLHNJqryDgWBlaDu3856lok/6iLSUREMtIehIiIZKQ9CBERyUgBISIiGSkgREQkIwWEiIhkpIAQEZGM/j+RprCGqAVELgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hisvrr8y4JXc",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate M2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEGjRVr1RDek",
        "colab_type": "code",
        "outputId": "a04e0b28-25ec-43ef-922b-f68ff8d10d06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        }
      },
      "source": [
        "y_dim = 10\n",
        "z_dim = 32\n",
        "h_dim = [256, 128]\n",
        "model = DeepGenerativeModel([784, y_dim, z_dim, h_dim])\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:112: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeepGenerativeModel(\n",
              "  (encoder): Encoder(\n",
              "    (hidden): ModuleList(\n",
              "      (0): Linear(in_features=794, out_features=256, bias=True)\n",
              "      (1): Linear(in_features=256, out_features=128, bias=True)\n",
              "    )\n",
              "    (sample): GaussianSample(\n",
              "      (mu): Linear(in_features=128, out_features=32, bias=True)\n",
              "      (log_var): Linear(in_features=128, out_features=32, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (hidden): ModuleList(\n",
              "      (0): Linear(in_features=42, out_features=128, bias=True)\n",
              "      (1): Linear(in_features=128, out_features=256, bias=True)\n",
              "    )\n",
              "    (reconstruction): Linear(in_features=256, out_features=784, bias=True)\n",
              "    (output_activation): Sigmoid()\n",
              "  )\n",
              "  (classifier): Classifier(\n",
              "    (dense): Linear(in_features=784, out_features=256, bias=True)\n",
              "    (logits): Linear(in_features=256, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgVZAt_kOfA5",
        "colab_type": "code",
        "outputId": "7378475c-851c-411e-abb8-8a25e48976e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Only use N labelled examples per class\n",
        "# The rest of the data is unlabelled.\n",
        "sampler = ImportanceWeightedSampler(mc=1, iw=1)\n",
        "elbo = SVI(model, likelihood=binary_cross_entropy, sampler=sampler)\n",
        "\n",
        "# number of labelled instances per class\n",
        "N_labels = [10, 50, 100, 200, 300, 400, 500]\n",
        "BATCH_SIZE = 32\n",
        "M2_accuracys = []\n",
        "\n",
        "for N in N_labels:\n",
        "    model = DeepGenerativeModel([784, y_dim, z_dim, h_dim])   \n",
        "\n",
        "    labelled, unlabelled, validation = get_mnist(location=\"./\", batch_size=BATCH_SIZE, labels_per_class=N)\n",
        "    alpha = 0.1 * len(unlabelled) / len(labelled)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.999))\n",
        "    print(\"Label per class = \", N)\n",
        "\n",
        "    for epoch in range(5):\n",
        "        # tell the model to train\n",
        "        model.train()\n",
        "        total_loss, accuracy = (0, 0)\n",
        "\n",
        "        labelled_iter = iter(labelled)\n",
        "      \n",
        "       # making sure we've seen all labelled cases:\n",
        "       # if no labelled case iter until we see one\n",
        "        for (u, _) in unlabelled:\n",
        "            labelled_elem = next(labelled_iter, None)\n",
        "            if labelled_elem is None:\n",
        "                labelled_iter = iter(labelled)\n",
        "                labelled_elem = next(labelled_iter, None)\n",
        "            (x, y) = labelled_elem\n",
        "\n",
        "        # for (x, y), (u, _) in zip(iter(labelled), iter(unlabelled)):\n",
        "        #     # maybe bounded by the shortest, need to maintain a separate iterator for the labelled data\n",
        "        #     # forward + backward + optimize\n",
        "        #     # Wrap in variables\n",
        "            x, y, u = Variable(x), Variable(y), Variable(u)\n",
        "            \n",
        "            L = -elbo(x, y)\n",
        "            U = -elbo(u) # using unlabelled data\n",
        "            # Add auxiliary classification loss q(y|x)\n",
        "            logits = model.classify(x)\n",
        "            \n",
        "            # Regular cross entropy\n",
        "            classication_loss = torch.sum(y * torch.log(logits + 1e-8), dim=1).mean()\n",
        "\n",
        "            J_alpha = L - alpha * classication_loss + U\n",
        "\n",
        "            J_alpha.backward()\n",
        "            # update params\n",
        "            optimizer.step()\n",
        "\n",
        "            # clear params \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            total_loss += torch.Tensor.item(J_alpha.data)\n",
        "            accuracy += torch.mean((torch.max(logits, 1)[1].data == torch.max(y, 1)[1].data).float())\n",
        "        \n",
        "        # end of each epoch \n",
        "        if epoch % 1 == 0:\n",
        "            # tell the model to test\n",
        "            model.eval()\n",
        "            m = len(unlabelled)\n",
        "            print(\"training\")\n",
        "            print(\"Epoch: {}\".format(epoch+1))\n",
        "            print(\"[Train]\\t\\t J_a: {:.2f}, accuracy: {:.2f}\".format(total_loss / m, accuracy / m))\n",
        "\n",
        "            total_loss, accuracy = (0, 0)\n",
        "\n",
        "            for x, y in validation:\n",
        "                x, y = Variable(x), Variable(y)\n",
        "                L = -elbo(x, y)\n",
        "                U = -elbo(x)\n",
        "\n",
        "                logits = model.classify(x)\n",
        "                classication_loss = -torch.sum(y * torch.log(logits + 1e-8), dim=1).mean()\n",
        "                J_alpha = L + alpha * classication_loss + U\n",
        "                total_loss += torch.Tensor.item(J_alpha.data)\n",
        "                accuracy += torch.mean((torch.max(logits, 1)[1].data == torch.max(y, 1)[1].data).float())\n",
        "\n",
        "            m = len(validation)\n",
        "            print(\"validation\")\n",
        "            print(\"[Validation]\\t J_a: {:.2f}, accuracy: {:.2f}\".format(total_loss / m, accuracy / m ))\n",
        "            M2_accuracys.append(accuracy / m)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:112: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Label per class =  10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training\n",
            "Epoch: 1\n",
            "[Train]\t\t J_a: 1127.95, accuracy: 0.99\n",
            "validation\n",
            "[Validation]\t J_a: 1170.17, accuracy: 0.74\n",
            "training\n",
            "Epoch: 2\n",
            "[Train]\t\t J_a: 1123.75, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1176.50, accuracy: 0.74\n",
            "training\n",
            "Epoch: 3\n",
            "[Train]\t\t J_a: 1123.61, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1181.50, accuracy: 0.73\n",
            "training\n",
            "Epoch: 4\n",
            "[Train]\t\t J_a: 1123.54, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1186.49, accuracy: 0.74\n",
            "training\n",
            "Epoch: 5\n",
            "[Train]\t\t J_a: 1123.56, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1190.69, accuracy: 0.74\n",
            "Label per class =  50\n",
            "training\n",
            "Epoch: 1\n",
            "[Train]\t\t J_a: 1125.35, accuracy: 0.97\n",
            "validation\n",
            "[Validation]\t J_a: 1130.03, accuracy: 0.86\n",
            "training\n",
            "Epoch: 2\n",
            "[Train]\t\t J_a: 1123.67, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1130.60, accuracy: 0.87\n",
            "training\n",
            "Epoch: 3\n",
            "[Train]\t\t J_a: 1123.67, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1131.75, accuracy: 0.87\n",
            "training\n",
            "Epoch: 4\n",
            "[Train]\t\t J_a: 1123.54, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1132.13, accuracy: 0.87\n",
            "training\n",
            "Epoch: 5\n",
            "[Train]\t\t J_a: 1123.58, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1132.78, accuracy: 0.86\n",
            "Label per class =  100\n",
            "training\n",
            "Epoch: 1\n",
            "[Train]\t\t J_a: 1124.94, accuracy: 0.95\n",
            "validation\n",
            "[Validation]\t J_a: 1125.83, accuracy: 0.90\n",
            "training\n",
            "Epoch: 2\n",
            "[Train]\t\t J_a: 1123.78, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1125.96, accuracy: 0.90\n",
            "training\n",
            "Epoch: 3\n",
            "[Train]\t\t J_a: 1123.60, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1126.14, accuracy: 0.90\n",
            "training\n",
            "Epoch: 4\n",
            "[Train]\t\t J_a: 1123.73, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1126.35, accuracy: 0.91\n",
            "training\n",
            "Epoch: 5\n",
            "[Train]\t\t J_a: 1123.73, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1126.35, accuracy: 0.91\n",
            "Label per class =  200\n",
            "training\n",
            "Epoch: 1\n",
            "[Train]\t\t J_a: 1124.46, accuracy: 0.93\n",
            "validation\n",
            "[Validation]\t J_a: 1124.67, accuracy: 0.91\n",
            "training\n",
            "Epoch: 2\n",
            "[Train]\t\t J_a: 1123.84, accuracy: 0.99\n",
            "validation\n",
            "[Validation]\t J_a: 1124.81, accuracy: 0.92\n",
            "training\n",
            "Epoch: 3\n",
            "[Train]\t\t J_a: 1123.73, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1124.65, accuracy: 0.92\n",
            "training\n",
            "Epoch: 4\n",
            "[Train]\t\t J_a: 1123.70, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1124.83, accuracy: 0.92\n",
            "training\n",
            "Epoch: 5\n",
            "[Train]\t\t J_a: 1123.72, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1124.75, accuracy: 0.92\n",
            "Label per class =  300\n",
            "training\n",
            "Epoch: 1\n",
            "[Train]\t\t J_a: 1124.25, accuracy: 0.92\n",
            "validation\n",
            "[Validation]\t J_a: 1124.26, accuracy: 0.92\n",
            "training\n",
            "Epoch: 2\n",
            "[Train]\t\t J_a: 1123.83, accuracy: 0.99\n",
            "validation\n",
            "[Validation]\t J_a: 1124.21, accuracy: 0.93\n",
            "training\n",
            "Epoch: 3\n",
            "[Train]\t\t J_a: 1123.72, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1124.18, accuracy: 0.94\n",
            "training\n",
            "Epoch: 4\n",
            "[Train]\t\t J_a: 1123.75, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1124.16, accuracy: 0.94\n",
            "training\n",
            "Epoch: 5\n",
            "[Train]\t\t J_a: 1123.62, accuracy: 1.00\n",
            "validation\n",
            "[Validation]\t J_a: 1124.25, accuracy: 0.94\n",
            "Label per class =  400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDv7sC2ylJWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "M2_accuracys = np.array([float(i.numpy()) for i in M2_accuracys])\n",
        "M2_err = 1-M2_accuracys[np.arange(4,len(M2_accuracys),5)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP0hcu-mHrKJ",
        "colab_type": "code",
        "outputId": "fe0dae4d-17e8-45c4-f32f-e3647ad415ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "N_labels = [10, 50, 100, 200, 300, 400, 500]\n",
        "N_labels_scaled = [i*10 for i in N_labels] # 10 for each class so total number of labelled samples should times 10\n",
        "plt.plot(N_labels_scaled, M2_err, label='M2 error')\n",
        "plt.plot(N_labels_scaled, M1_err, label='M1 error')\n",
        "plt.xlabel(\"labelled samples\")\n",
        "plt.ylabel(\"testing error\")\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb0e70f9160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}